{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reynarajkumar/verdictvision-legal-ai/blob/main/Copy_of_DL_CMPE_258_Verdict_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiBfjpZN914a"
      },
      "source": [
        "#STEP 1: Data Acquisition – Download California CaseLaw Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClpzuUho5TUn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "LAST_VOL = 251        # highest volume available\n",
        "COUNT = 10            # how many volumes to download\n",
        "START_VOL = LAST_VOL - COUNT + 1  # 242 → 251\n",
        "OUTPUT_DIR = \"/content/all_cases_json\"\n",
        "BASE_ROOT = \"https://static.case.law/cal-rptr-3d/{}/cases/\"\n",
        "# ============================\n",
        "\n",
        "# Make output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def get_soup(url):\n",
        "    r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "global_index = 1\n",
        "\n",
        "for vol in range(START_VOL, LAST_VOL + 1):\n",
        "    url = BASE_ROOT.format(vol)\n",
        "    print(f\"\\n===== Volume {vol} =====\")\n",
        "    print(f\"Fetching: {url}\")\n",
        "\n",
        "    try:\n",
        "        soup = get_soup(url)\n",
        "    except Exception as e:\n",
        "        print(f\"  !! Cannot load volume {vol}: {e}\")\n",
        "        continue\n",
        "\n",
        "    json_files = sorted(\n",
        "        href for a in soup.find_all(\"a\", href=True)\n",
        "        if (href := a[\"href\"].lower()).endswith(\".json\")\n",
        "    )\n",
        "\n",
        "    print(f\"  Found {len(json_files)} JSON files\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        file_url = urljoin(url, json_file)\n",
        "        dest_path = os.path.join(OUTPUT_DIR, f\"{global_index}.json\")\n",
        "\n",
        "        print(f\"    {global_index}: {file_url}\")\n",
        "\n",
        "        try:\n",
        "            r = requests.get(file_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "            r.raise_for_status()\n",
        "            with open(dest_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            global_index += 1\n",
        "        except Exception as e:\n",
        "            print(f\"      !! Failed: {e}\")\n",
        "\n",
        "print(\"\\nDone! Files saved in:\", OUTPUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPUaGElj-YgN"
      },
      "source": [
        "#STEP 2: Export Downloded Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSKOCK9L_d2G"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Folder where the JSON files were saved in the previous cell\n",
        "SOURCE_DIR = \"/content/all_cases_json\"\n",
        "\n",
        "# Path for the zip file\n",
        "ZIP_PATH = \"/content/all_cases_json_zip\"\n",
        "\n",
        "# Create ZIP (this makes all_cases_json_zip.zip)\n",
        "shutil.make_archive(ZIP_PATH, 'zip', SOURCE_DIR)\n",
        "\n",
        "print(\"Created ZIP at:\", ZIP_PATH + \".zip\")\n",
        "\n",
        "files.download(\"/content/all_cases_json_zip.zip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnUJrLB5_U_N"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8Mnw9PJADBQ"
      },
      "source": [
        "#Step 3 : Download and Extract case_law dataset from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gQNqjq4_A_h"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "FILE_ID = \"1KvKVVkCfpxordzChDezxB3G3JNDdv6zh\"\n",
        "OUTPUT_ZIP = \"/content/case_law.zip\"\n",
        "EXTRACT_DIR = \"/content/case_law\"\n",
        "# ==============\n",
        "\n",
        "# Create direct download URL\n",
        "url = f\"https://drive.google.com/uc?id={FILE_ID}\"\n",
        "\n",
        "# Download the ZIP\n",
        "gdown.download(url, OUTPUT_ZIP, quiet=False)\n",
        "\n",
        "# Make extraction directory\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP\n",
        "with zipfile.ZipFile(OUTPUT_ZIP, \"r\") as zf:\n",
        "    zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "print(\"Done! Extracted to:\", EXTRACT_DIR)\n",
        "print(\"Top-level contents:\", os.listdir(EXTRACT_DIR))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLR_4SUK-beD"
      },
      "source": [
        "#STEP 4: DATA LOADING & VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95bSMZUF-GIL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_and_validate_cases(input_dir='/content/case_law'):\n",
        "    \"\"\"\n",
        "    Load JSON case files and perform initial validation\n",
        "    \"\"\"\n",
        "    # Check if directory exists\n",
        "    if not os.path.exists(input_dir):\n",
        "        print(f\"Error: Directory '{input_dir}' not found!\")\n",
        "        print(f\"\\nPlease ensure your JSON files are in: {input_dir}\")\n",
        "        return []\n",
        "\n",
        "    # Get all JSON files\n",
        "    json_files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n",
        "\n",
        "    print(f\"\\nInput directory: {input_dir}\")\n",
        "    print(f\"Found {len(json_files)} JSON files\")\n",
        "\n",
        "    if len(json_files) == 0:\n",
        "        print(\"No JSON files found!\")\n",
        "        return []\n",
        "\n",
        "    # Load each case\n",
        "    cases = []\n",
        "    errors = []\n",
        "\n",
        "    print(f\"\\nLoading cases...\")\n",
        "\n",
        "    for i, filename in enumerate(json_files, 1):\n",
        "        filepath = os.path.join(input_dir, filename)\n",
        "\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                case = json.load(f)\n",
        "\n",
        "                # Basic validation\n",
        "                if 'id' in case and 'casebody' in case:\n",
        "                    cases.append(case)\n",
        "                    if i % 10 == 0:\n",
        "                        print(f\"  Loaded {i}/{len(json_files)} files...\")\n",
        "                else:\n",
        "                    errors.append(f\"{filename}: Missing required fields\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            errors.append(f\"{filename}: Invalid JSON - {e}\")\n",
        "        except Exception as e:\n",
        "            errors.append(f\"{filename}: {e}\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"LOADING SUMMARY\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Successfully loaded: {len(cases)} cases\")\n",
        "\n",
        "    if errors:\n",
        "        print(f\"Errors: {len(errors)}\")\n",
        "        print(\"\\nFirst 3 errors:\")\n",
        "        for err in errors[:3]:\n",
        "            print(f\"  - {err}\")\n",
        "\n",
        "    # Show sample case structure\n",
        "    if cases:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"SAMPLE CASE STRUCTURE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        sample = cases[0]\n",
        "        print(f\"Case ID: {sample.get('id', 'N/A')}\")\n",
        "        print(f\"Case Name: {sample.get('name', 'N/A')[:60]}...\")\n",
        "        print(f\"Decision Date: {sample.get('decision_date', 'N/A')}\")\n",
        "        print(f\"Court: {sample.get('court', {}).get('name', 'N/A') if isinstance(sample.get('court'), dict) else sample.get('court', 'N/A')}\")\n",
        "        print(f\"\\nTop-level keys: {list(sample.keys())}\")\n",
        "\n",
        "        if 'casebody' in sample:\n",
        "            print(f\"Casebody keys: {list(sample['casebody'].keys())}\")\n",
        "\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    return cases\n",
        "\n",
        "# Load all cases from your directory\n",
        "cases = load_and_validate_cases('/content/all_cases_json')\n",
        "\n",
        "# Display statistics\n",
        "if cases:\n",
        "    print(f\"\\nSuccessfully loaded {len(cases)} cases!\")\n",
        "    print(f\"\\nQuick Statistics:\")\n",
        "\n",
        "    # Count cases with opinions\n",
        "    with_opinions = sum(1 for c in cases if 'casebody' in c and 'opinions' in c['casebody'])\n",
        "    print(f\"  Cases with opinions: {with_opinions}\")\n",
        "\n",
        "    # Count cases with citations\n",
        "    with_citations = sum(1 for c in cases if 'citations' in c and c['citations'])\n",
        "    print(f\"  Cases with citations: {with_citations}\")\n",
        "\n",
        "    # Show date range\n",
        "    dates = [c.get('decision_date', '') for c in cases if c.get('decision_date')]\n",
        "    if dates:\n",
        "        dates_sorted = sorted(dates)\n",
        "        print(f\"  Date range: {dates_sorted[0]} to {dates_sorted[-1]}\")\n",
        "\n",
        "    print(f\"\\nData loaded successfully! Ready for Step 2 (Text Extraction)\")\n",
        "else:\n",
        "    print(\"\\nNo cases loaded. Please check your input directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMFLnKwM-igT"
      },
      "source": [
        "#Step 5: Text Extraction & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QMx8iu9-iLf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "#  TEXT EXTRACTION & CLEANING\n",
        "# Input: 713 loaded cases\n",
        "# Output: Structured data with clean text\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_metadata(case):\n",
        "    \"\"\"Extract structured metadata\"\"\"\n",
        "    metadata = {\n",
        "        'case_id': case.get('id', ''),\n",
        "        'case_name': case.get('name', ''),\n",
        "        'name_abbreviation': case.get('name_abbreviation', ''),\n",
        "        'decision_date': case.get('decision_date', ''),\n",
        "        'docket_number': case.get('docket_number', ''),\n",
        "    }\n",
        "\n",
        "    # Court\n",
        "    court = case.get('court', {})\n",
        "    metadata['court_name'] = court.get('name', '') if isinstance(court, dict) else str(court)\n",
        "\n",
        "    # Jurisdiction\n",
        "    jurisdiction = case.get('jurisdiction', {})\n",
        "    metadata['jurisdiction'] = jurisdiction.get('name', '') if isinstance(jurisdiction, dict) else str(jurisdiction)\n",
        "\n",
        "    # Citations\n",
        "    citations = []\n",
        "    if 'citations' in case:\n",
        "        for cite in case['citations']:\n",
        "            if isinstance(cite, dict) and 'cite' in cite:\n",
        "                citations.append(cite['cite'])\n",
        "            elif isinstance(cite, str):\n",
        "                citations.append(cite)\n",
        "    metadata['citations'] = citations\n",
        "\n",
        "    # Judges from casebody\n",
        "    judges = []\n",
        "    if 'casebody' in case and 'judges' in case['casebody']:\n",
        "        judges = case['casebody']['judges']\n",
        "        if not isinstance(judges, list):\n",
        "            judges = [judges]\n",
        "    metadata['judges'] = judges\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def extract_full_text(case):\n",
        "    \"\"\"Extract complete case text from JSON structure\"\"\"\n",
        "    text_parts = []\n",
        "\n",
        "    # Case name\n",
        "    if 'name' in case:\n",
        "        text_parts.append(f\"CASE: {case['name']}\")\n",
        "\n",
        "    # Citations\n",
        "    if 'citations' in case:\n",
        "        cites = [c.get('cite', '') if isinstance(c, dict) else str(c)\n",
        "                for c in case['citations']]\n",
        "        if cites:\n",
        "            text_parts.append(f\"CITATIONS: {', '.join(cites)}\")\n",
        "\n",
        "    # Casebody\n",
        "    if 'casebody' in case:\n",
        "        casebody = case['casebody']\n",
        "\n",
        "        # Head matter (summary)\n",
        "        if 'head_matter' in casebody and casebody['head_matter']:\n",
        "            text_parts.append(f\"SUMMARY:\\n{casebody['head_matter']}\")\n",
        "\n",
        "        # Parties\n",
        "        if 'parties' in casebody and casebody['parties']:\n",
        "            for party in casebody['parties']:\n",
        "                party_text = party if isinstance(party, str) else party.get('text', '')\n",
        "                if party_text:\n",
        "                    text_parts.append(f\"PARTY: {party_text}\")\n",
        "\n",
        "        # Attorneys\n",
        "        if 'attorneys' in casebody and casebody['attorneys']:\n",
        "            for attorney in casebody['attorneys']:\n",
        "                atty_text = attorney if isinstance(attorney, str) else attorney.get('text', '')\n",
        "                if atty_text:\n",
        "                    text_parts.append(f\"ATTORNEY: {atty_text}\")\n",
        "\n",
        "        # Judges\n",
        "        if 'judges' in casebody and casebody['judges']:\n",
        "            judges_str = ', '.join([str(j) for j in casebody['judges']])\n",
        "            text_parts.append(f\"JUDGES: {judges_str}\")\n",
        "\n",
        "        # Opinions (MAIN CONTENT)\n",
        "        if 'opinions' in casebody and casebody['opinions']:\n",
        "            for i, opinion in enumerate(casebody['opinions']):\n",
        "                if isinstance(opinion, dict):\n",
        "                    if 'type' in opinion:\n",
        "                        text_parts.append(f\"OPINION TYPE: {opinion['type']}\")\n",
        "                    if 'author' in opinion:\n",
        "                        text_parts.append(f\"AUTHOR: {opinion['author']}\")\n",
        "                    if 'text' in opinion and opinion['text']:\n",
        "                        text_parts.append(f\"OPINION {i+1}:\\n{opinion['text']}\")\n",
        "\n",
        "    return '\\n\\n'.join([str(part) for part in text_parts if part])\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove page numbers and artifacts\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'\\*\\d+', '', text)\n",
        "\n",
        "    # Normalize quotes\n",
        "    text = text.replace('\"', '\"').replace('\"', '\"')\n",
        "    text = text.replace(''', \"'\").replace(''', \"'\")\n",
        "\n",
        "    # Normalize newlines\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def extract_outcome(text):\n",
        "    \"\"\"Extract outcome label for classification\"\"\"\n",
        "    if not text or len(text) < 50:\n",
        "        return 'other'\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    if 'affirm' in text_lower and 'not affirm' not in text_lower:\n",
        "        return 'affirmed'\n",
        "    elif 'revers' in text_lower:\n",
        "        return 'reversed'\n",
        "    elif 'remand' in text_lower:\n",
        "        return 'remanded'\n",
        "    elif 'dismiss' in text_lower:\n",
        "        return 'dismissed'\n",
        "    elif 'modified' in text_lower:\n",
        "        return 'modified'\n",
        "    else:\n",
        "        return 'other'\n",
        "\n",
        "# ============================================================================\n",
        "# PROCESS ALL CASES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 2: TEXT EXTRACTION & CLEANING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "processed_data = []\n",
        "\n",
        "print(f\"\\nProcessing {len(cases)} cases...\")\n",
        "\n",
        "for i, case in enumerate(cases):\n",
        "    try:\n",
        "        # Extract components\n",
        "        metadata = extract_metadata(case)\n",
        "        full_text = extract_full_text(case)\n",
        "        clean_text_result = clean_text(full_text)\n",
        "        outcome = extract_outcome(clean_text_result)\n",
        "\n",
        "        # Combine\n",
        "        case_data = {\n",
        "            **metadata,\n",
        "            'full_text': clean_text_result,\n",
        "            'text_length': len(clean_text_result),\n",
        "            'word_count': len(clean_text_result.split()),\n",
        "            'num_citations': len(metadata['citations']),\n",
        "            'num_judges': len(metadata['judges']),\n",
        "            'outcome_label': outcome\n",
        "        }\n",
        "\n",
        "        processed_data.append(case_data)\n",
        "\n",
        "        # Progress indicator\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  Processed {i + 1}/{len(cases)} cases...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error processing case {i}: {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(processed_data)\n",
        "\n",
        "# Display summary\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"EXTRACTION SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total cases processed: {len(df)}\")\n",
        "print(f\"\\nText Statistics:\")\n",
        "print(f\"  Average length: {df['text_length'].mean():.0f} characters\")\n",
        "print(f\"  Min length: {df['text_length'].min():.0f} characters\")\n",
        "print(f\"  Max length: {df['text_length'].max():.0f} characters\")\n",
        "print(f\"  Average word count: {df['word_count'].mean():.0f} words\")\n",
        "\n",
        "print(f\"\\nCitation Statistics:\")\n",
        "print(f\"  Average citations: {df['num_citations'].mean():.1f}\")\n",
        "print(f\"  Cases with citations: {len(df[df['num_citations'] > 0])}\")\n",
        "\n",
        "print(f\"\\nOutcome Distribution:\")\n",
        "print(df['outcome_label'].value_counts())\n",
        "\n",
        "print(f\"\\nData Quality:\")\n",
        "print(f\"  Cases with >1000 chars: {len(df[df['text_length'] > 1000])}\")\n",
        "print(f\"  Cases with >5000 chars: {len(df[df['text_length'] > 5000])}\")\n",
        "print(f\"  Cases with >10000 chars: {len(df[df['text_length'] > 10000])}\")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample Processed Case:\")\n",
        "sample = df.iloc[0]\n",
        "print(f\"Case: {sample['case_name'][:60]}...\")\n",
        "print(f\"Outcome: {sample['outcome_label']}\")\n",
        "print(f\"Text length: {sample['text_length']} chars\")\n",
        "print(f\"Citations: {len(sample['citations'])}\")\n",
        "print(f\"\\nText preview:\")\n",
        "print(sample['full_text'][:300] + \"...\")\n",
        "\n",
        "print(f\"\\n completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGbT_5KsA09u"
      },
      "source": [
        "#Step 6: Text Chunking for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA6SbfMYAvIB"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "#  TEXT CHUNKING FOR RAG\n",
        "# Create overlapping chunks from long documents\n",
        "# ============================================================================\n",
        "\n",
        "def create_text_chunks(text, chunk_size=500, overlap=100):\n",
        "    \"\"\"\n",
        "    Create overlapping text chunks for RAG retrieval\n",
        "\n",
        "    Parameters:\n",
        "    - chunk_size: 500 words (optimal for embeddings)\n",
        "    - overlap: 100 words (preserves context across chunks)\n",
        "    \"\"\"\n",
        "    if not text or len(text) < 100:\n",
        "        return [{'text': text, 'chunk_id': 0, 'length': len(text.split())}]\n",
        "\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk_words = words[i:i + chunk_size]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "\n",
        "        chunks.append({\n",
        "            'text': chunk_text,\n",
        "            'chunk_id': len(chunks),\n",
        "            'start_idx': i,\n",
        "            'length': len(chunk_words)\n",
        "        })\n",
        "\n",
        "        # Stop if we've processed all words\n",
        "        if len(chunk_words) < chunk_size:\n",
        "            break\n",
        "\n",
        "    return chunks if chunks else [{'text': text, 'chunk_id': 0, 'length': len(text.split())}]\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE CHUNKS FOR ALL CASES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 3: TEXT CHUNKING FOR RAG\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_chunks = []\n",
        "total_chunks = 0\n",
        "\n",
        "print(f\"\\nCreating chunks for {len(df)} cases...\")\n",
        "print(\"Chunk size: 500 words\")\n",
        "print(\"Overlap: 100 words\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # Create chunks for this case\n",
        "    chunks = create_text_chunks(row['full_text'], chunk_size=500, overlap=100)\n",
        "\n",
        "    # Add metadata to each chunk\n",
        "    for chunk in chunks:\n",
        "        all_chunks.append({\n",
        "            'case_id': row['case_id'],\n",
        "            'case_name': row['case_name'],\n",
        "            'court': row['court_name'],\n",
        "            'decision_date': row['decision_date'],\n",
        "            'citations': row['citations'],\n",
        "            'judges': row['judges'],\n",
        "            'outcome': row['outcome_label'],\n",
        "            'chunk_id': chunk['chunk_id'],\n",
        "            'text': chunk['text'],\n",
        "            'chunk_length': chunk['length']\n",
        "        })\n",
        "\n",
        "    total_chunks += len(chunks)\n",
        "\n",
        "    # Progress\n",
        "    if (idx + 1) % 100 == 0:\n",
        "        print(f\"  Processed {idx + 1}/{len(df)} cases... ({total_chunks} chunks so far)\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CHUNKING SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total cases: {len(df)}\")\n",
        "print(f\"Total chunks created: {len(all_chunks)}\")\n",
        "print(f\"Average chunks per case: {len(all_chunks) / len(df):.1f}\")\n",
        "\n",
        "# Chunk statistics\n",
        "chunk_lengths = [c['chunk_length'] for c in all_chunks]\n",
        "print(f\"\\nChunk Statistics:\")\n",
        "print(f\"  Average chunk size: {sum(chunk_lengths) / len(chunk_lengths):.0f} words\")\n",
        "print(f\"  Min chunk size: {min(chunk_lengths)} words\")\n",
        "print(f\"  Max chunk size: {max(chunk_lengths)} words\")\n",
        "\n",
        "# Sample chunks\n",
        "print(f\"\\nSample Chunks:\")\n",
        "for i in range(min(3, len(all_chunks))):\n",
        "    chunk = all_chunks[i]\n",
        "    print(f\"\\n  Chunk {i+1}:\")\n",
        "    print(f\"    Case: {chunk['case_name'][:50]}...\")\n",
        "    print(f\"    Length: {chunk['chunk_length']} words\")\n",
        "    print(f\"    Text: {chunk['text'][:150]}...\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Step 3 complete!\")\n",
        "print(f\"Created {len(all_chunks)} chunks for RAG retrieval\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY9jgK5VBGVa"
      },
      "source": [
        "#Step 7: Save All Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwmDzfdVBDvA"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SAVE PREPROCESSED DATA\n",
        "# Save in multiple formats for different components\n",
        "# ============================================================================\n",
        "\n",
        "import json\n",
        "\n",
        "output_dir = '/content/verdictvision_preprocessed'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\" SAVING PREPROCESSED DATA\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nOutput directory: {output_dir}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 1. SAVE METADATA CSV (for quick analysis)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Saving metadata CSV\")\n",
        "\n",
        "metadata_df = df[['case_id', 'case_name', 'court_name', 'decision_date',\n",
        "                   'num_citations', 'num_judges', 'outcome_label',\n",
        "                   'text_length', 'word_count']].copy()\n",
        "\n",
        "metadata_df.to_csv(f'{output_dir}/cases_metadata.csv', index=False)\n",
        "print(f\" Saved: cases_metadata.csv ({len(metadata_df)} cases)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. SAVE FULL DATA JSON (complete case data)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[2] Saving full case data JSON.\")\n",
        "\n",
        "df_json = df.to_dict('records')\n",
        "with open(f'{output_dir}/cases_full.json', 'w') as f:\n",
        "    json.dump(df_json, f, indent=2)\n",
        "\n",
        "print(f\" Saved: cases_full.json ({len(df_json)} cases)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. SAVE TEXT CHUNKS JSON (for RAG retrieval)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[3] Saving text chunks JSON\")\n",
        "\n",
        "with open(f'{output_dir}/text_chunks.json', 'w') as f:\n",
        "    json.dump(all_chunks, f, indent=2)\n",
        "\n",
        "print(f\" Saved: text_chunks.json ({len(all_chunks)} chunks)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. SAVE CLASSIFICATION DATA CSV (for model training)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"[4] Saving classification data CSV\")\n",
        "\n",
        "classification_df = df[['case_id', 'full_text', 'outcome_label',\n",
        "                        'num_citations', 'text_length', 'word_count']].copy()\n",
        "\n",
        "classification_df.to_csv(f'{output_dir}/classification_data.csv', index=False)\n",
        "print(f\"Saved: classification_data.csv ({len(classification_df)} cases)\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PREPROCESSING COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nFinal Statistics:\")\n",
        "print(f\"  Total cases: {len(df)}\")\n",
        "print(f\"  Total chunks: {len(all_chunks)}\")\n",
        "print(f\"  Chunks per case: {len(all_chunks) / len(df):.1f}\")\n",
        "print(f\"  Avg text length: {df['text_length'].mean():.0f} chars\")\n",
        "print(f\"  Avg word count: {df['word_count'].mean():.0f} words\")\n",
        "\n",
        "print(f\"\\nOutput Files:\")\n",
        "print(f\"  {output_dir}/\")\n",
        "print(f\"  ├── cases_metadata.csv       ({len(df)} rows)\")\n",
        "print(f\"  ├── cases_full.json          ({len(df)} cases)\")\n",
        "print(f\"  ├── text_chunks.json         ({len(all_chunks)} chunks)\")\n",
        "print(f\"  └── classification_data.csv  ({len(df)} rows)\")\n",
        "\n",
        "print(f\"\\nOutcome Distribution:\")\n",
        "outcome_counts = df['outcome_label'].value_counts()\n",
        "for outcome, count in outcome_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"  {outcome:12s}: {count:4d} ({percentage:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nAll preprocessed data saved to: {output_dir}\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5DzpCNNLmZ6"
      },
      "source": [
        "#Step 8: Complete Retrieval System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVRLRmEJLXlc"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BUILD RETRIEVAL SYSTEM\n",
        "# Components: Embeddings, Vector Search, Keyword Search, Weighted Fusion\n",
        "# ============================================================================\n",
        "\n",
        "!pip install sentence-transformers scikit-learn -q\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [1] LOAD PREPROCESSED DATA\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[1] Loading preprocessed chunks...\")\n",
        "with open('/content/verdictvision_preprocessed/text_chunks.json', 'r') as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "texts = [chunk['text'] for chunk in chunks]\n",
        "print(f\" Loaded {len(chunks)} chunks\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [2] CREATE EMBEDDINGS (sentence-transformers)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[2] Creating embeddings with sentence-transformers...\")\n",
        "print(\"  Model: joshcx/static-embedding-all-MiniLM-L6-v2 (256 dimensions)\")\n",
        "\n",
        "# Use the static embedding model from Hugging Face\n",
        "embedder = SentenceTransformer(\"joshcx/static-embedding-all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create embeddings for all chunks\n",
        "embeddings = embedder.encode(\n",
        "    texts,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True  # For cosine similarity\n",
        ")\n",
        "\n",
        "print(f\"  Embeddings created: {embeddings.shape}\")\n",
        "print(f\"    {len(texts)} texts → {embeddings.shape[1]}-dimensional vectors\")\n",
        "\n",
        "# Save embeddings\n",
        "np.save('/content/verdictvision_preprocessed/embeddings.npy', embeddings)\n",
        "print(f\"  Saved embeddings.npy\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [3] CREATE TF-IDF INDEX (Keyword Search)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[3] Creating TF-IDF index for keyword search...\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=2000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words='english',\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "print(f\"   TF-IDF matrix: {tfidf_matrix.shape}\")\n",
        "print(f\"    Vocabulary: {len(vectorizer.vocabulary_)} terms\")\n",
        "\n",
        "# Save TF-IDF\n",
        "import scipy.sparse\n",
        "scipy.sparse.save_npz('/content/verdictvision_preprocessed/tfidf_matrix.npz', tfidf_matrix)\n",
        "\n",
        "import pickle\n",
        "with open('/content/verdictvision_preprocessed/tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(f\"  Saved TF-IDF index\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [4] IMPLEMENT HYBRID SEARCH WITH WEIGHTED FUSION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[4] Implementing hybrid search function...\")\n",
        "\n",
        "def hybrid_search(query, top_k=5,\n",
        "                  semantic_weight=0.6,\n",
        "                  keyword_weight=0.3,\n",
        "                  metadata_weight=0.1):\n",
        "    \"\"\"\n",
        "    FIXED: Returns top-k UNIQUE CASES (no duplicates)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"HYBRID SEARCH: '{query}'\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # 1. SEMANTIC SEARCH\n",
        "    print(f\"\\n[1] Vector search...\")\n",
        "    query_embedding = embedder.encode(query, normalize_embeddings=True)\n",
        "    semantic_scores = cosine_similarity(\n",
        "        query_embedding.reshape(1, -1),\n",
        "        embeddings\n",
        "    )[0]\n",
        "    print(f\" Semantic: [{semantic_scores.min():.3f}, {semantic_scores.max():.3f}]\")\n",
        "\n",
        "    # 2. KEYWORD SEARCH\n",
        "    print(f\"[2/4] Keyword search\")\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    keyword_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
        "    print(f\"   Keyword: [{keyword_scores.min():.3f}, {keyword_scores.max():.3f}]\")\n",
        "\n",
        "    # 3. IMPROVED METADATA SCORING\n",
        "    print(f\"[3] Metadata scoring.\")\n",
        "\n",
        "    def compute_metadata_score(chunk):\n",
        "        \"\"\"Multi-factor metadata scoring\"\"\"\n",
        "        score = 0.0\n",
        "\n",
        "        # Citation count (more = better)\n",
        "        citations = chunk.get('citations', [])\n",
        "        if citations:\n",
        "            score += min(len(citations) / 5.0, 1.0) * 0.5\n",
        "\n",
        "        # Outcome clarity\n",
        "        outcome = chunk.get('outcome', 'other')\n",
        "        if outcome in ['affirmed', 'reversed', 'remanded']:\n",
        "            score += 0.3\n",
        "        elif outcome == 'modified':\n",
        "            score += 0.15\n",
        "\n",
        "        # Judge count\n",
        "        judges = chunk.get('judges', [])\n",
        "        if judges:\n",
        "            score += min(len(judges) / 3.0, 1.0) * 0.2\n",
        "\n",
        "        return score\n",
        "\n",
        "    metadata_scores = np.array([compute_metadata_score(c) for c in chunks])\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    if metadata_scores.max() > 0:\n",
        "        metadata_scores = metadata_scores / metadata_scores.max()\n",
        "\n",
        "    print(f\" Metadata: [{metadata_scores.min():.3f}, {metadata_scores.max():.3f}]\")\n",
        "\n",
        "    # 4. WEIGHTED FUSION\n",
        "    print(f\"[4] Fusion (weights: {semantic_weight}/{keyword_weight}/{metadata_weight})...\")\n",
        "\n",
        "    final_scores = (\n",
        "        semantic_weight * semantic_scores +\n",
        "        keyword_weight * keyword_scores +\n",
        "        metadata_weight * metadata_scores\n",
        "    )\n",
        "\n",
        "    print(f\"  Final: [{final_scores.min():.3f}, {final_scores.max():.3f}]\")\n",
        "\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "    #  AGGREGATE BY CASE_ID (NO DUPLICATES)\n",
        "    # ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "    print(f\"\\n[5] Aggregating by case (removing duplicates)...\")\n",
        "\n",
        "    # Dictionary: case_id -> best chunk for that case\n",
        "    case_best = {}\n",
        "\n",
        "    for idx in range(len(chunks)):\n",
        "        case_id = chunks[idx].get('case_id', f\"UNKNOWN_{idx}\")\n",
        "        score = final_scores[idx]\n",
        "\n",
        "        # Keep only the highest-scoring chunk per case\n",
        "        if case_id not in case_best or score > case_best[case_id]['score']:\n",
        "            case_best[case_id] = {\n",
        "                'score': score,\n",
        "                'chunk_idx': idx,\n",
        "                'case_name': chunks[idx]['case_name'],\n",
        "                'court': chunks[idx]['court'],\n",
        "                'citations': chunks[idx].get('citations', []),\n",
        "                'outcome': chunks[idx]['outcome'],\n",
        "                'decision_date': chunks[idx].get('decision_date', ''),\n",
        "                'text': chunks[idx]['text'],\n",
        "                'semantic_score': semantic_scores[idx],\n",
        "                'keyword_score': keyword_scores[idx],\n",
        "                'metadata_score': metadata_scores[idx]\n",
        "            }\n",
        "\n",
        "    # Sort cases by score\n",
        "    sorted_cases = sorted(case_best.values(), key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    # Take top_k UNIQUE cases\n",
        "    top_cases = sorted_cases[:top_k]\n",
        "\n",
        "    print(f\" Found {len(case_best)} unique cases, returning top {len(top_cases)}\")\n",
        "\n",
        "    # Format results\n",
        "    results = []\n",
        "    for rank, case in enumerate(top_cases, 1):\n",
        "        results.append({\n",
        "            'rank': rank,\n",
        "            'case_name': case['case_name'],\n",
        "            'court': case['court'],\n",
        "            'citations': case['citations'],\n",
        "            'outcome': case['outcome'],\n",
        "            'decision_date': case['decision_date'],\n",
        "            'text_snippet': case['text'][:400],\n",
        "            'scores': {\n",
        "                'final': float(case['score']),\n",
        "                'semantic': float(case['semantic_score']),\n",
        "                'keyword': float(case['keyword_score']),\n",
        "                'metadata': float(case['metadata_score'])\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"  Hybrid search function ready\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# TEST HYBRID SEARCH\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TESTING HYBRID SEARCH\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_query = \"breach of contract damages\"\n",
        "results = hybrid_search(test_query, top_k=5)\n",
        "\n",
        "print(f\"\\nTop 5 Results:\")\n",
        "for r in results:\n",
        "    print(f\"\\n{r['rank']}. {r['case_name']}\")\n",
        "    print(f\"   Final Score: {r['scores']['final']:.4f}\")\n",
        "    print(f\"   - Semantic: {r['scores']['semantic']:.4f}\")\n",
        "    print(f\"   - Keyword:  {r['scores']['keyword']:.4f}\")\n",
        "    print(f\"   - Metadata: {r['scores']['metadata']:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\" Complete!\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsd_XkDj0CC-"
      },
      "source": [
        "# Step 9 : TWO-STAGE RETRIEVAL WITH STATIC EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaqVfTWc1pec"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "#  TWO-STAGE RETRIEVAL WITH STATIC EMBEDDINGS\n",
        "# ============================================================================\n",
        "\n",
        "!pip install gensim -q\n",
        "\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 5B: TWO-STAGE RETRIEVAL SYSTEM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [1] LOAD STATIC EMBEDDINGS WITH RETRY LOGIC\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[1] Loading static embeddings...\")\n",
        "\n",
        "def load_glove_with_retry(model_name, max_retries=3):\n",
        "    \"\"\"Load GloVe model with retry logic for network issues\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  Attempt {attempt + 1}/{max_retries}: Loading '{model_name}'...\")\n",
        "            model = api.load(model_name)\n",
        "            print(f\"  ✓ Successfully loaded!\")\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Attempt {attempt + 1} failed: {str(e)[:50]}...\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"  Retrying in 5 seconds...\")\n",
        "                time.sleep(5)\n",
        "    return None\n",
        "\n",
        "# Try loading models in order of preference (smaller = more reliable)\n",
        "glove_model = None\n",
        "\n",
        "# Option 1: Try the 50-dim model first (smaller, faster download ~65MB)\n",
        "print(\"\\nTrying GloVe 50-dim (smaller, more reliable)...\")\n",
        "glove_model = load_glove_with_retry('glove-wiki-gigaword-50', max_retries=3)\n",
        "glove_dim = 50\n",
        "\n",
        "# Option 2: If 50-dim fails, try 100-dim\n",
        "if glove_model is None:\n",
        "    print(\"\\nTrying GloVe 100-dim...\")\n",
        "    glove_model = load_glove_with_retry('glove-wiki-gigaword-100', max_retries=2)\n",
        "    glove_dim = 100\n",
        "\n",
        "# Option 3: Fallback to Word2Vec (different source)\n",
        "if glove_model is None:\n",
        "    print(\"\\nTrying Word2Vec as fallback...\")\n",
        "    try:\n",
        "        glove_model = api.load('word2vec-google-news-300')\n",
        "        glove_dim = 300\n",
        "        print(\"  Loaded Word2Vec 300-dim\")\n",
        "    except:\n",
        "        print(\"  ✗ Word2Vec also failed\")\n",
        "\n",
        "# Option 4: Use TF-IDF based approach if all embedding downloads fail\n",
        "if glove_model is None:\n",
        "    print(\"\\n All embedding downloads failed. Using TF-IDF for Stage 1 instead.\")\n",
        "    USE_TFIDF_STAGE1 = True\n",
        "else:\n",
        "    USE_TFIDF_STAGE1 = False\n",
        "    print(f\"\\n Loaded static embeddings: {len(glove_model)} words, {glove_dim} dimensions\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [2] CREATE STATIC EMBEDDINGS FOR CHUNKS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[2] Creating static embeddings for chunks...\")\n",
        "\n",
        "if not USE_TFIDF_STAGE1:\n",
        "    def get_static_embedding(text, model, dim):\n",
        "        \"\"\"Create document embedding by averaging word vectors\"\"\"\n",
        "        words = text.lower().split()\n",
        "        vectors = []\n",
        "        for word in words:\n",
        "            if word in model:\n",
        "                vectors.append(model[word])\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        return np.zeros(dim)\n",
        "\n",
        "    # Create static embeddings for all chunks\n",
        "    print(\"  Creating embeddings for all chunks...\")\n",
        "    static_embeddings = np.array([\n",
        "        get_static_embedding(chunk['text'], glove_model, glove_dim)\n",
        "        for chunk in chunks\n",
        "    ])\n",
        "\n",
        "    # Normalize for cosine similarity\n",
        "    norms = np.linalg.norm(static_embeddings, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1\n",
        "    static_embeddings_norm = static_embeddings / norms\n",
        "\n",
        "    print(f\" Created static embeddings: {static_embeddings_norm.shape}\")\n",
        "\n",
        "    # Save\n",
        "    np.save('/content/verdictvision_preprocessed/static_embeddings.npy', static_embeddings_norm)\n",
        "    print(\"  Saved static_embeddings.npy\")\n",
        "\n",
        "else:\n",
        "    # Use TF-IDF matrix as \"static\" stage 1 (already computed in Step 5)\n",
        "    print(\"  Using TF-IDF matrix for Stage 1 filtering\")\n",
        "    static_embeddings_norm = None\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [3] IMPLEMENT TWO-STAGE SEARCH FUNCTION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[3] Implementing two-stage retrieval function...\")\n",
        "\n",
        "def two_stage_search(query, stage1_k=100, final_k=5,\n",
        "                     semantic_weight=0.6,\n",
        "                     keyword_weight=0.3,\n",
        "                     metadata_weight=0.1):\n",
        "    \"\"\"\n",
        "    Two-stage retrieval:\n",
        "    Stage 1: Fast filtering (GloVe or TF-IDF) → top stage1_k candidates\n",
        "    Stage 2: Precision reranking with transformer → top final_k results\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TWO-STAGE SEARCH: '{query}'\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # ========== STAGE 1: Fast Filtering ==========\n",
        "    print(f\"\\n[Stage 1] Fast filtering (top {stage1_k})...\")\n",
        "    stage1_start = time.time()\n",
        "\n",
        "    if USE_TFIDF_STAGE1:\n",
        "        # Use TF-IDF for stage 1\n",
        "        query_tfidf = vectorizer.transform([query])\n",
        "        static_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
        "    else:\n",
        "        # Use GloVe for stage 1\n",
        "        query_static = get_static_embedding(query, glove_model, glove_dim)\n",
        "        query_static_norm = query_static / (np.linalg.norm(query_static) + 1e-10)\n",
        "        static_scores = np.dot(static_embeddings_norm, query_static_norm)\n",
        "\n",
        "    # Get top stage1_k candidates\n",
        "    stage1_indices = np.argsort(static_scores)[::-1][:stage1_k]\n",
        "    stage1_time = time.time() - stage1_start\n",
        "\n",
        "    print(f\" Stage 1 complete: {stage1_time*1000:.2f}ms\")\n",
        "\n",
        "    # ========== STAGE 2: Precision Reranking ==========\n",
        "    print(f\"\\n[Stage 2] Reranking with transformer...\")\n",
        "    stage2_start = time.time()\n",
        "\n",
        "    # Transformer embedding for query\n",
        "    query_transformer = embedder.encode(query, normalize_embeddings=True)\n",
        "\n",
        "    # Only compute for stage1 candidates\n",
        "    candidate_transformer_embeddings = embeddings[stage1_indices]\n",
        "    transformer_scores = np.dot(candidate_transformer_embeddings, query_transformer)\n",
        "\n",
        "    # TF-IDF for candidates\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    candidate_tfidf = tfidf_matrix[stage1_indices]\n",
        "    keyword_scores = cosine_similarity(query_tfidf, candidate_tfidf)[0]\n",
        "\n",
        "    # Metadata scores\n",
        "    candidate_metadata = np.array([\n",
        "        len(chunks[idx].get('citations', [])) for idx in stage1_indices\n",
        "    ], dtype=float)\n",
        "    if candidate_metadata.max() > 0:\n",
        "        candidate_metadata /= candidate_metadata.max()\n",
        "\n",
        "    # Weighted fusion\n",
        "    final_scores = (semantic_weight * transformer_scores +\n",
        "                   keyword_weight * keyword_scores +\n",
        "                   metadata_weight * candidate_metadata)\n",
        "\n",
        "    stage2_time = time.time() - stage2_start\n",
        "    print(f\" Stage 2 complete: {stage2_time*1000:.2f}ms\")\n",
        "\n",
        "    # ========== DEDUPLICATE BY CASE ==========\n",
        "    case_best = {}\n",
        "    for i, idx in enumerate(stage1_indices):\n",
        "        case_id = chunks[idx].get('case_id', f\"UNKNOWN_{idx}\")\n",
        "        score = final_scores[i]\n",
        "\n",
        "        if case_id not in case_best or score > case_best[case_id]['score']:\n",
        "            case_best[case_id] = {\n",
        "                'score': score,\n",
        "                'chunk_idx': idx,\n",
        "                'case_name': chunks[idx]['case_name'],\n",
        "                'court': chunks[idx]['court'],\n",
        "                'citations': chunks[idx].get('citations', []),\n",
        "                'outcome': chunks[idx]['outcome'],\n",
        "                'text': chunks[idx]['text'],\n",
        "                'transformer_score': transformer_scores[i],\n",
        "                'keyword_score': keyword_scores[i],\n",
        "                'static_score': static_scores[idx]\n",
        "            }\n",
        "\n",
        "    sorted_cases = sorted(case_best.values(), key=lambda x: x['score'], reverse=True)\n",
        "    top_cases = sorted_cases[:final_k]\n",
        "\n",
        "    total_time = stage1_time + stage2_time\n",
        "    print(f\"\\n Total time: {total_time*1000:.2f}ms\")\n",
        "    print(f\" Returning {len(top_cases)} unique cases\")\n",
        "\n",
        "    # Format results\n",
        "    results = []\n",
        "    for rank, case in enumerate(top_cases, 1):\n",
        "        results.append({\n",
        "            'rank': rank,\n",
        "            'case_name': case['case_name'],\n",
        "            'court': case['court'],\n",
        "            'citations': case['citations'],\n",
        "            'outcome': case['outcome'],\n",
        "            'text_snippet': case['text'][:400],\n",
        "            'scores': {\n",
        "                'final': float(case['score']),\n",
        "                'transformer': float(case['transformer_score']),\n",
        "                'keyword': float(case['keyword_score']),\n",
        "                'static': float(case['static_score'])\n",
        "            },\n",
        "            'timing': {\n",
        "                'stage1_ms': stage1_time * 1000,\n",
        "                'stage2_ms': stage2_time * 1000,\n",
        "                'total_ms': total_time * 1000\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\" Two-stage search function ready\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [4] BENCHMARK: TWO-STAGE vs FULL HYBRID\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BENCHMARKING: TWO-STAGE vs FULL HYBRID SEARCH\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_queries = [\n",
        "    \"breach of contract damages California\",\n",
        "    \"negligence standard of care duty\",\n",
        "    \"premises liability slip and fall\",\n",
        "    \"employment wrongful termination\",\n",
        "    \"fraud misrepresentation elements\"\n",
        "]\n",
        "\n",
        "full_hybrid_times = []\n",
        "two_stage_times = []\n",
        "overlap_scores = []\n",
        "\n",
        "print(\"\\nRunning benchmark on 5 test queries...\\n\")\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"Query {i}: '{query[:40]}...'\")\n",
        "\n",
        "    # Full hybrid search timing\n",
        "    start = time.time()\n",
        "    full_results = hybrid_search(query, top_k=5)\n",
        "    full_time = time.time() - start\n",
        "    full_hybrid_times.append(full_time)\n",
        "\n",
        "    # Two-stage search timing\n",
        "    start = time.time()\n",
        "    two_stage_results = two_stage_search(query, stage1_k=100, final_k=5)\n",
        "    two_stage_time = time.time() - start\n",
        "    two_stage_times.append(two_stage_time)\n",
        "\n",
        "    # Calculate overlap\n",
        "    full_cases = set([r['case_name'] for r in full_results])\n",
        "    two_stage_cases = set([r['case_name'] for r in two_stage_results])\n",
        "    overlap = len(full_cases & two_stage_cases) / 5\n",
        "    overlap_scores.append(overlap)\n",
        "\n",
        "    print(f\"  Full: {full_time*1000:.1f}ms | Two-Stage: {two_stage_time*1000:.1f}ms | Overlap: {overlap*100:.0f}%\\n\")\n",
        "\n",
        "# Summary\n",
        "print(\"=\"*70)\n",
        "print(\"BENCHMARK SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "avg_full = np.mean(full_hybrid_times) * 1000\n",
        "avg_two_stage = np.mean(two_stage_times) * 1000\n",
        "speedup = avg_full / avg_two_stage if avg_two_stage > 0 else 1\n",
        "\n",
        "print(f\"\\n{'Metric':<25} {'Full Hybrid':>15} {'Two-Stage':>15}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'Avg Latency':<25} {avg_full:>12.1f} ms {avg_two_stage:>12.1f} ms\")\n",
        "print(f\"{'Speedup':<25} {'1.0x':>15} {speedup:>14.2f}x\")\n",
        "print(f\"{'Avg Result Overlap':<25} {'100%':>15} {np.mean(overlap_scores)*100:>14.0f}%\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Step 5B Complete!\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# VISUALIZATION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# Plot 1: Latency per query\n",
        "ax1 = axes[0]\n",
        "x = range(1, len(test_queries) + 1)\n",
        "ax1.plot(x, [t*1000 for t in full_hybrid_times], 'o-', label='Full Hybrid', linewidth=2)\n",
        "ax1.plot(x, [t*1000 for t in two_stage_times], 's-', label='Two-Stage', linewidth=2)\n",
        "ax1.set_xlabel('Query')\n",
        "ax1.set_ylabel('Latency (ms)')\n",
        "ax1.set_title('Latency per Query')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Average Comparison\n",
        "ax2 = axes[1]\n",
        "methods = ['Full\\nHybrid', 'Two-Stage']\n",
        "times = [avg_full, avg_two_stage]\n",
        "colors = ['#3498db', '#2ecc71']\n",
        "bars = ax2.bar(methods, times, color=colors)\n",
        "ax2.set_ylabel('Avg Latency (ms)')\n",
        "ax2.set_title(f'Average Latency\\n(Speedup: {speedup:.2f}x)')\n",
        "for bar, t in zip(bars, times):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "             f'{t:.1f}ms', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Plot 3: Quality (Overlap)\n",
        "ax3 = axes[2]\n",
        "ax3.bar(range(1, len(overlap_scores)+1), [o*100 for o in overlap_scores], color='#9b59b6')\n",
        "ax3.axhline(y=np.mean(overlap_scores)*100, color='red', linestyle='--',\n",
        "            label=f'Avg: {np.mean(overlap_scores)*100:.0f}%')\n",
        "ax3.set_xlabel('Query')\n",
        "ax3.set_ylabel('Result Overlap (%)')\n",
        "ax3.set_title('Quality: Result Overlap')\n",
        "ax3.set_ylim(0, 105)\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/verdictvision_preprocessed/two_stage_benchmark.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Visualization saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHYeqjTeu3CF"
      },
      "outputs": [],
      "source": [
        "for r in results:\n",
        "    print(\"====\", r[\"case_name\"])\n",
        "    print(r[\"text_snippet\"])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdwvkHja0aR8"
      },
      "source": [
        "#Step 10: COMPLETE VERDICTVISION APP (Q&A + IRAC DRAFTING + OUTCOME PREDICTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aqG9DT4mzXA"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPLETE VERDICTVISION APP (Q&A + IRAC DRAFTING + OUTCOME PREDICTION)\n",
        "# Includes: LLM loading, Q&A system, IRAC drafting, LLM outcome prediction,\n",
        "#           unified Gradio UI\n",
        "# ============================================================================\n",
        "\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INITIALIZING COMPLETE VERDICTVISION APP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [1] LOAD LLM (Phi-2)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[1] Loading LLM (Microsoft Phi-2)...\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Check GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"  Device: {device}\")\n",
        "\n",
        "# Load Phi-2 model\n",
        "model_name = \"microsoft/phi-2\"\n",
        "\n",
        "print(f\"  Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "print(f\"  Loading model (this may take 1-2 minutes)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(f\" LLM loaded successfully!\")\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [2/4] DEFINE Q&A SYSTEM CLASS (your existing logic, unchanged)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[2] Defining Q&A system class...\")\n",
        "\n",
        "class VerdictVisionQA:\n",
        "    \"\"\"Fixed Q&A system with correct embedding handling\"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.embedder = embedder  # From Step 5\n",
        "        self.embeddings = embeddings  # From Step 5\n",
        "        self.chunks = chunks\n",
        "        self.vectorizer = vectorizer\n",
        "        self.tfidf_matrix = tfidf_matrix\n",
        "        print(f\"  Q&A initialized with {len(chunks)} chunks\")\n",
        "        print(f\"  Embedding shape: {self.embeddings.shape}\")\n",
        "\n",
        "    def retrieve_cases(self, query, top_k=3):\n",
        "        \"\"\"RAG: Hybrid search (60/30/10)\"\"\"\n",
        "\n",
        "        #  embedding - no [0] indexing\n",
        "        query_emb = self.embedder.encode(query, normalize_embeddings=True)\n",
        "        # query_emb shape is (384,) for all-MiniLM-L6-v2\n",
        "\n",
        "        # Compute semantic similarity\n",
        "        semantic_scores = cosine_similarity(\n",
        "            query_emb.reshape(1, -1),  # (1, 384)\n",
        "            self.embeddings             # (n_chunks, 384)\n",
        "        )[0]\n",
        "\n",
        "        # Keyword (TF-IDF)\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        keyword_scores = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]\n",
        "\n",
        "        # Metadata\n",
        "        metadata_scores = np.array([\n",
        "            0.5 if c.get('citations') else 0.0 for c in self.chunks\n",
        "        ])\n",
        "        if metadata_scores.max() > 0:\n",
        "            metadata_scores /= metadata_scores.max()\n",
        "\n",
        "        # Fusion (60% semantic, 30% keyword, 10% metadata)\n",
        "        final_scores = 0.6 * semantic_scores + 0.3 * keyword_scores + 0.1 * metadata_scores\n",
        "\n",
        "        # Top-k\n",
        "        top_indices = np.argsort(final_scores)[::-1][:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append({\n",
        "                'case_name': self.chunks[idx]['case_name'],\n",
        "                'court': self.chunks[idx]['court'],\n",
        "                'citations': self.chunks[idx].get('citations', []),\n",
        "                'outcome': self.chunks[idx]['outcome'],\n",
        "                'text': self.chunks[idx]['text'][:500],\n",
        "                'score': float(final_scores[idx])\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_answer(self, question, cases):\n",
        "        \"\"\"Generate answer with LLM\"\"\"\n",
        "        prompt = \"You are a legal assistant. Answer based on these California cases.\\n\\n\"\n",
        "\n",
        "        for i, case in enumerate(cases, 1):\n",
        "            prompt += f\"Case {i}: {case['case_name']}\\n\"\n",
        "            prompt += f\"Court: {case['court']}\\n\"\n",
        "            prompt += f\"Key text: {case['text'][:200]}...\\n\\n\"\n",
        "\n",
        "        prompt += f\"Question: {question}\\n\\nAnswer (cite cases):\"\n",
        "\n",
        "        try:\n",
        "            output = self.llm(prompt, max_new_tokens=150, return_full_text=False)\n",
        "            answer = output[0]['generated_text'].strip()\n",
        "            answer = answer.split('\\n')[0]\n",
        "            if len(answer) < 30:\n",
        "                answer = f\"Based on {cases[0]['case_name']}, \" + answer\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            print(f\"LLM Error: {e}\")\n",
        "            return f\"Based on {cases[0]['case_name']}, the California courts have addressed this issue.\"\n",
        "\n",
        "    def format_response(self, answer, cases):\n",
        "        \"\"\"Format with citations\"\"\"\n",
        "        result = f\"**💬 Answer:**\\n\\n{answer}\\n\\n\"\n",
        "        result += \"---\\n\\n\"\n",
        "        result += f\"**📚 Sources ({len(cases)} Cases):**\\n\\n\"\n",
        "\n",
        "        for i, case in enumerate(cases, 1):\n",
        "            result += f\"**{i}. {case['case_name']}**\\n\"\n",
        "            result += f\"   • Court: {case['court']}\\n\"\n",
        "            if case['citations']:\n",
        "                result += f\"   • Citation: {case['citations'][0]}\\n\"\n",
        "            result += f\"   • Outcome: {case['outcome']}\\n\"\n",
        "            result += f\"   • Relevance: {case['score']:.1%}\\n\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"Complete Q&A pipeline\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"User: {question}\")\n",
        "\n",
        "        print(\"[1] RAG retrieval...\")\n",
        "        cases = self.retrieve_cases(question, top_k=3)\n",
        "\n",
        "        print(\"[2] LLM generation...\")\n",
        "        answer = self.generate_answer(question, cases)\n",
        "\n",
        "        print(\"[3] Adding citations...\")\n",
        "        result = self.format_response(answer, cases)\n",
        "\n",
        "        print(\"Complete\\n\")\n",
        "        return result\n",
        "\n",
        "    def respond(self, user_message, chat_history):\n",
        "        \"\"\"Gradio chatbot respond function\"\"\"\n",
        "        if not user_message.strip():\n",
        "            return \"\", chat_history\n",
        "\n",
        "        try:\n",
        "            answer = self.answer_question(user_message)\n",
        "            chat_history.append((user_message, answer))\n",
        "            return \"\", chat_history\n",
        "        except Exception as e:\n",
        "            error_msg = f\" Error: {str(e)}\\n\\nPlease try rephrasing your question.\"\n",
        "            chat_history.append((user_message, error_msg))\n",
        "            return \"\", chat_history\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "#  DEFINE IRAC DRAFTING ASSISTANT CLASS (using hybrid_search from Step 5)\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n Defining IRAC drafting assistant class\")\n",
        "\n",
        "class VerdictVisionDraftingIRAC:\n",
        "    \"\"\"\n",
        "    Drafting assistant for:\n",
        "      - Case briefs\n",
        "      - Legal memos\n",
        "      - Motions / arguments\n",
        "\n",
        "    Uses:\n",
        "      - hybrid_search (Step 5) for retrieval\n",
        "      - llm_pipeline (Phi-2) for generation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.chunks = chunks\n",
        "        print(\"  ✓ IRAC drafting assistant initialized\")\n",
        "\n",
        "    # ---------------- Retrieval ---------------- #\n",
        "\n",
        "    def retrieve_supporting_cases(self, description, top_k=5):\n",
        "        \"\"\"\n",
        "        Use the case description as a query into the hybrid retriever.\n",
        "        Returns top_k supporting cases.\n",
        "        \"\"\"\n",
        "        query = f\"facts: {description}\"\n",
        "        results = hybrid_search(query, top_k=top_k)  # assumes hybrid_search from Step 5\n",
        "\n",
        "        cases = []\n",
        "        for r in results:\n",
        "            cases.append({\n",
        "                \"case_name\": r[\"case_name\"],\n",
        "                \"court\": r[\"court\"],\n",
        "                \"citations\": r.get(\"citations\", []),\n",
        "                \"outcome\": r.get(\"outcome\", \"unknown\"),\n",
        "                \"text_snippet\": r.get(\"text_snippet\", r.get(\"text\", \"\")),\n",
        "                \"score\": float(r[\"scores\"][\"final\"])\n",
        "            })\n",
        "        return cases\n",
        "\n",
        "    # ---------------- Prompt Building ---------------- #\n",
        "\n",
        "    def build_prompt(self, description, doc_type, tone, cases, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Build the LLM prompt for IRAC-style drafting with APA-style citations.\n",
        "        Includes the selected document type explicitly.\n",
        "        \"\"\"\n",
        "\n",
        "        tone_map = {\n",
        "            \"Neutral / Objective\": \"neutral, objective, and analytical\",\n",
        "            \"Persuasive / Advocacy\": \"persuasive and advocacy-oriented, but still professional\",\n",
        "            \"Plain-language summary\": \"plain language suitable for a non-lawyer client\"\n",
        "        }\n",
        "        tone_instruction = tone_map.get(tone, \"neutral, objective, and analytical\")\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a California legal writing assistant.\\n\"\n",
        "            \"Draft a legal document using IRAC format (FACTS, ISSUE, RULE, ANALYSIS, CONCLUSION)\\n\"\n",
        "            \"based on the CASE DESCRIPTION and SUPPORTING CASES below.\\n\\n\"\n",
        "            f\"REQUESTED DOCUMENT TYPE: {doc_type}\\n\"\n",
        "            f\"TONE: {tone_instruction}\\n\\n\"\n",
        "            \"CASE DESCRIPTION (provided by the user):\\n\"\n",
        "            f\"{description}\\n\\n\"\n",
        "            \"SUPPORTING CASES (for legal context; you may cite them in the ANALYSIS):\\n\"\n",
        "        )\n",
        "\n",
        "        # IMPORTANT: truncate excerpts so the prompt isn’t huge (helps avoid CUDA OOM)\n",
        "        for i, case in enumerate(cases, 1):\n",
        "            excerpt = case[\"text_snippet\"][:400]  # truncate to ~400 chars\n",
        "            prompt += f\"Case {i}: {case['case_name']}\\n\"\n",
        "            prompt += f\"  Court: {case['court']}\\n\"\n",
        "            if case[\"citations\"]:\n",
        "                prompt += f\"  Citation: {case['citations'][0]}\\n\"\n",
        "            prompt += f\"  Outcome: {case['outcome']}\\n\"\n",
        "            prompt += f\"  Excerpt: {excerpt}...\\n\\n\"\n",
        "\n",
        "        prompt += (\n",
        "            \"DRAFTING INSTRUCTIONS (MANDATORY):\\n\"\n",
        "            \"1. Structure the document using EXACTLY these headings, in all caps:\\n\"\n",
        "            \"   FACTS\\n\"\n",
        "            \"   ISSUE\\n\"\n",
        "            \"   RULE\\n\"\n",
        "            \"   ANALYSIS\\n\"\n",
        "            \"   CONCLUSION\\n\"\n",
        "            \"2. In the FACTS section, summarize the key facts from the case description.\\n\"\n",
        "            \"3. In the ISSUE section, state the central legal question as a single, clear sentence.\\n\"\n",
        "            \"4. In the RULE section, state the relevant California legal standard(s).\\n\"\n",
        "            \"5. In the ANALYSIS section:\\n\"\n",
        "            \"   - Apply the rule to the specific facts.\\n\"\n",
        "            \"   - Refer to supporting cases using APA-style parenthetical citations.\\n\"\n",
        "            \"   - Example format: (Omlansky v. Save Mart Supermarkets, 2019).\\n\"\n",
        "            \"6. In the CONCLUSION section, provide a brief, reasoned conclusion \"\n",
        "            \"about how the court is likely to view the claim.\\n\"\n",
        "            \"7. Do NOT include a separate 'Sources', 'References', or 'Supporting Cases' section \"\n",
        "            \"inside the draft. That will be provided separately.\\n\"\n",
        "            \"8. Use clear, complete sentences and organized paragraphs. Avoid bullet points.\\n\\n\"\n",
        "            \"NOW WRITE THE DRAFT, USING ONLY THE IRAC HEADINGS (FACTS, ISSUE, RULE, ANALYSIS, CONCLUSION):\\n\"\n",
        "        )\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    # ---------------- Draft Generation ---------------- #\n",
        "\n",
        "    def generate_draft(self, description, doc_type, tone, approx_length_tokens):\n",
        "        \"\"\"\n",
        "        High-level function: retrieve cases, build prompt, call LLM, format result.\n",
        "        Returns:\n",
        "          - draft_text (IRAC-only, with doc_type as a heading)\n",
        "          - sources_text (supporting cases list)\n",
        "        \"\"\"\n",
        "        description = (description or \"\").strip()\n",
        "        if not description:\n",
        "            return \"Please provide a case description.\", \"\"\n",
        "\n",
        "        # 1. Retrieve supporting cases\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"DRAFTING REQUEST\")\n",
        "        print(f\"Document type: {doc_type}\")\n",
        "        print(f\"Tone: {tone}\")\n",
        "        print(\"[1/3] Retrieving supporting cases...\")\n",
        "        cases = self.retrieve_supporting_cases(description, top_k=5)\n",
        "\n",
        "        # 2. Build prompt\n",
        "        print(\"[2/3] Building IRAC + APA prompt...\")\n",
        "\n",
        "        # Make generation lighter to avoid CUDA OOM: cap at 256 tokens\n",
        "        max_new_tokens = int(min(max(approx_length_tokens, 128), 256))\n",
        "\n",
        "        prompt = self.build_prompt(description, doc_type, tone, cases, max_new_tokens)\n",
        "\n",
        "        # 3. Call LLM (with basic CUDA OOM fallback)\n",
        "        print(\"[3/3] Generating draft with LLM...\")\n",
        "        try:\n",
        "            output = self.llm(\n",
        "                prompt,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                return_full_text=False\n",
        "            )\n",
        "            generated = output[0][\"generated_text\"].strip()\n",
        "        except RuntimeError as e:\n",
        "            if \"CUDA out of memory\" in str(e):\n",
        "                print(\" CUDA OOM during drafting. Retrying with fewer tokens (128).\")\n",
        "                try:\n",
        "                    output = self.llm(\n",
        "                        prompt,\n",
        "                        max_new_tokens=128,\n",
        "                        return_full_text=False\n",
        "                    )\n",
        "                    generated = output[0][\"generated_text\"].strip()\n",
        "                except Exception as e2:\n",
        "                    generated = f\"Error during generation after CUDA OOM retry: {e2}\"\n",
        "            else:\n",
        "                generated = f\"Error during generation: {e}\"\n",
        "        except Exception as e:\n",
        "            generated = f\"Error during generation: {e}\"\n",
        "\n",
        "        # Prepend the selected document type to the draft so it's obvious\n",
        "        draft = f\"# {doc_type}\\n\\n\" + generated\n",
        "\n",
        "        # Build sources text for the UI (separate from draft)\n",
        "        sources_lines = []\n",
        "        for i, case in enumerate(cases, 1):\n",
        "            line = f\"{i}. {case['case_name']} ({case['court']})\"\n",
        "            if case[\"citations\"]:\n",
        "                line += f\"\\n   Citation: {case['citations'][0]}\"\n",
        "            line += f\"\\n   Outcome: {case['outcome']}\"\n",
        "            line += f\"\\n   Relative score (0–1): {case['score']:.3f}\\n\"\n",
        "            sources_lines.append(line)\n",
        "\n",
        "        sources_text = \"\\n\".join(sources_lines)\n",
        "\n",
        "        print(\"Drafting complete.\\n\")\n",
        "        return draft, sources_text\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# LLM-BASED OUTCOME PREDICTION\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n Defining LLM-based outcome predictor...\")\n",
        "\n",
        "class LLMOutcomePredictor:\n",
        "    \"\"\"\n",
        "    LLM-based outcome prediction using RAG + reasoning.\n",
        "\n",
        "    Advantages over ML classifier:\n",
        "    1. Provides explainable reasoning\n",
        "    2. Leverages semantic understanding of legal concepts\n",
        "    3. Uses retrieved precedents for context\n",
        "    4. No separate training required\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm, retriever_fn):\n",
        "        self.llm = llm\n",
        "        self.retrieve = retriever_fn\n",
        "        print(\"  ✓ LLM Outcome Predictor initialized\")\n",
        "\n",
        "    def predict(self, case_description, num_similar=5):\n",
        "        \"\"\"\n",
        "        Predict case outcome using LLM reasoning over retrieved precedents.\n",
        "\n",
        "        Returns:\n",
        "            dict with prediction, confidence, reasoning, and supporting cases\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Retrieve similar cases\n",
        "        print(\"\\n[1/3] Retrieving similar precedents...\")\n",
        "        start_time = time.time()\n",
        "        similar_cases = self.retrieve(case_description, top_k=num_similar)\n",
        "        retrieval_time = time.time() - start_time\n",
        "\n",
        "        # Step 2: Analyze outcome distribution\n",
        "        outcomes = [c.get('outcome', 'unknown').lower() for c in similar_cases]\n",
        "        outcome_dist = Counter(outcomes)\n",
        "\n",
        "        # Step 3: Build reasoning prompt\n",
        "        print(\"[2/3] Building reasoning prompt...\")\n",
        "        prompt = self._build_prompt(case_description, similar_cases, outcome_dist)\n",
        "\n",
        "        # Step 4: Get LLM prediction\n",
        "        print(\"[3/3] Generating prediction with LLM reasoning...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            output = self.llm(prompt, max_new_tokens=200, return_full_text=False)\n",
        "            response = output[0]['generated_text'].strip()\n",
        "            llm_time = time.time() - start_time\n",
        "\n",
        "            # Parse the response\n",
        "            prediction, confidence, reasoning = self._parse_response(response, outcome_dist)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ LLM error: {e}\")\n",
        "            llm_time = 0\n",
        "            # Fallback to majority voting\n",
        "            prediction = outcome_dist.most_common(1)[0][0] if outcome_dist else \"affirmed\"\n",
        "            confidence = \"medium\"\n",
        "            reasoning = \"Prediction based on majority outcome of similar cases (LLM fallback).\"\n",
        "\n",
        "        return {\n",
        "            'prediction': prediction.upper(),\n",
        "            'confidence': confidence,\n",
        "            'reasoning': reasoning,\n",
        "            'similar_cases': similar_cases,\n",
        "            'outcome_distribution': dict(outcome_dist),\n",
        "            'retrieval_time_ms': retrieval_time * 1000,\n",
        "            'llm_time_ms': llm_time * 1000\n",
        "        }\n",
        "\n",
        "    def _build_prompt(self, description, cases, outcome_dist):\n",
        "        \"\"\"Build structured prompt for LLM reasoning\"\"\"\n",
        "\n",
        "        prompt = \"\"\"You are an experienced California appellate court analyst.\n",
        "Analyze the case below and predict the most likely appellate outcome.\n",
        "\n",
        "═══════════════════════════════════════════════════════════════════\n",
        "CASE FACTS TO ANALYZE:\n",
        "═══════════════════════════════════════════════════════════════════\n",
        "\"\"\"\n",
        "        prompt += description[:600] + \"\\n\\n\"\n",
        "\n",
        "        prompt += \"\"\"═══════════════════════════════════════════════════════════════════\n",
        "SIMILAR PRECEDENT CASES:\n",
        "═══════════════════════════════════════════════════════════════════\n",
        "\"\"\"\n",
        "        for i, case in enumerate(cases[:5], 1):\n",
        "            prompt += f\"\\n{i}. {case.get('case_name', 'Unknown Case')[:80]}\\n\"\n",
        "            prompt += f\"   Outcome: {case.get('outcome', 'unknown')}\\n\"\n",
        "            prompt += f\"   Key facts: {case.get('text', '')[:150]}...\\n\"\n",
        "\n",
        "        prompt += f\"\"\"\n",
        "═══════════════════════════════════════════════════════════════════\n",
        "OUTCOME DISTRIBUTION OF SIMILAR CASES:\n",
        "═══════════════════════════════════════════════════════════════════\n",
        "\"\"\"\n",
        "        for outcome, count in outcome_dist.most_common():\n",
        "            prompt += f\"  • {outcome}: {count} case(s)\\n\"\n",
        "\n",
        "        prompt += \"\"\"\n",
        "═══════════════════════════════════════════════════════════════════\n",
        "INSTRUCTIONS:\n",
        "═══════════════════════════════════════════════════════════════════\n",
        "Based on the case facts and similar precedents:\n",
        "\n",
        "1. Identify the key legal issues\n",
        "2. Compare with precedent outcomes\n",
        "3. Predict the outcome: AFFIRMED, REVERSED, or REMANDED\n",
        "\n",
        "Respond in this EXACT format:\n",
        "PREDICTION: [AFFIRMED/REVERSED/REMANDED]\n",
        "CONFIDENCE: [HIGH/MEDIUM/LOW]\n",
        "REASONING: [2-3 sentences explaining your analysis]\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def _parse_response(self, response, outcome_dist):\n",
        "        \"\"\"Parse structured response from LLM\"\"\"\n",
        "\n",
        "        prediction = \"affirmed\"  # default\n",
        "        confidence = \"medium\"\n",
        "        reasoning = response\n",
        "\n",
        "        lines = response.split('\\n')\n",
        "        for line in lines:\n",
        "            line_lower = line.lower().strip()\n",
        "\n",
        "            if line_lower.startswith('prediction:'):\n",
        "                pred_text = line.split(':', 1)[-1].strip().lower()\n",
        "                if 'affirm' in pred_text:\n",
        "                    prediction = 'affirmed'\n",
        "                elif 'reverse' in pred_text:\n",
        "                    prediction = 'reversed'\n",
        "                elif 'remand' in pred_text:\n",
        "                    prediction = 'remanded'\n",
        "\n",
        "            elif line_lower.startswith('confidence:'):\n",
        "                conf_text = line.split(':', 1)[-1].strip().lower()\n",
        "                if 'high' in conf_text:\n",
        "                    confidence = 'high'\n",
        "                elif 'low' in conf_text:\n",
        "                    confidence = 'low'\n",
        "                else:\n",
        "                    confidence = 'medium'\n",
        "\n",
        "            elif line_lower.startswith('reasoning:'):\n",
        "                reasoning = line.split(':', 1)[-1].strip()\n",
        "\n",
        "        return prediction, confidence, reasoning\n",
        "\n",
        "\n",
        "print(\"\\n[2.7/4] Creating retrieval function for predictor...\")\n",
        "\n",
        "def retrieve_for_prediction(query, top_k=5):\n",
        "    \"\"\"Wrapper for simple hybrid search used in outcome prediction\"\"\"\n",
        "    query_emb = embedder.encode(query, normalize_embeddings=True)\n",
        "    semantic_scores = cosine_similarity(query_emb.reshape(1, -1), embeddings)[0]\n",
        "\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    keyword_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
        "\n",
        "    final_scores = 0.6 * semantic_scores + 0.3 * keyword_scores + 0.1 * 0\n",
        "\n",
        "    case_best = {}\n",
        "    for idx, score in enumerate(final_scores):\n",
        "        case_id = chunks[idx].get('case_id', f\"UNK_{idx}\")\n",
        "        if case_id not in case_best or score > case_best[case_id]['score']:\n",
        "            case_best[case_id] = {\n",
        "                'score': float(score),\n",
        "                'case_name': chunks[idx]['case_name'],\n",
        "                'text': chunks[idx]['text'][:400],\n",
        "                'outcome': chunks[idx].get('outcome', 'unknown'),\n",
        "                'court': chunks[idx].get('court', ''),\n",
        "                'citations': chunks[idx].get('citations', [])\n",
        "            }\n",
        "\n",
        "    sorted_results = sorted(case_best.values(), key=lambda x: x['score'], reverse=True)\n",
        "    return sorted_results[:top_k]\n",
        "\n",
        "print(\" Initializing LLM predictor...\")\n",
        "llm_predictor = LLMOutcomePredictor(llm_pipeline, retrieve_for_prediction)\n",
        "\n",
        "def predict_ui(case_description, num_cases):\n",
        "    \"\"\"Gradio UI wrapper for outcome prediction\"\"\"\n",
        "    if not case_description.strip():\n",
        "        return \"Please enter a case description.\", \"\", \"\"\n",
        "\n",
        "    result = llm_predictor.predict(case_description, num_similar=int(num_cases))\n",
        "\n",
        "    # Format prediction output\n",
        "    pred_text = f\"\"\"\n",
        "## 🎯 Predicted Outcome: **{result['prediction']}**\n",
        "\n",
        "**Confidence:** {result['confidence'].upper()}\n",
        "\n",
        "**Processing Time:**\n",
        "- Retrieval: {result['retrieval_time_ms']:.0f} ms\n",
        "- LLM Reasoning: {result['llm_time_ms']:.0f} ms\n",
        "\"\"\"\n",
        "\n",
        "    # Format reasoning\n",
        "    reason_text = f\"\"\"\n",
        "### 📝 LLM Reasoning\n",
        "\n",
        "{result['reasoning']}\n",
        "\n",
        "### 📊 Similar Case Outcomes\n",
        "\"\"\"\n",
        "    for outcome, count in result['outcome_distribution'].items():\n",
        "        reason_text += f\"- **{outcome}**: {count} case(s)\\n\"\n",
        "\n",
        "    # Format similar cases\n",
        "    cases_text = \"### 📚 Supporting Precedents\\n\\n\"\n",
        "    for i, case in enumerate(result['similar_cases'], 1):\n",
        "        cases_text += f\"**{i}. {case['case_name'][:70]}...**\\n\"\n",
        "        cases_text += f\"- Outcome: {case['outcome']}\\n\"\n",
        "        cases_text += f\"- Relevance Score: {case['score']:.3f}\\n\\n\"\n",
        "\n",
        "    return pred_text, reason_text, cases_text\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [3/4] INITIALIZE BOTH SYSTEMS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[3/4] Initializing systems...\")\n",
        "qa_system = VerdictVisionQA(llm_pipeline)\n",
        "drafting_system_irac = VerdictVisionDraftingIRAC(llm_pipeline)\n",
        "\n",
        "# Quick test for Q&A\n",
        "print(\"\\n  Testing Q&A retrieval...\")\n",
        "test_result = qa_system.retrieve_cases(\"breach of contract\", top_k=3)\n",
        "print(f\"  ✓ Retrieved {len(test_result)} cases successfully!\")\n",
        "print(f\"  ✓ First case: {test_result[0]['case_name'][:60]}...\")\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "# [4/4] UNIFIED GRADIO UI WITH THREE TABS\n",
        "# ═══════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print(\"\\n[4/4] Building unified Gradio interface (Q&A + IRAC + Prediction)...\")\n",
        "\n",
        "examples = [\n",
        "    \"What are the requirements for breach of contract?\",\n",
        "    \"What constitutes negligence in California?\",\n",
        "    \"What are the elements of fraud?\",\n",
        "    \"Can employers terminate without cause?\",\n",
        "    \"What damages are available for breach?\",\n",
        "    \"What is the statute of limitations for breach of contract in California?\"\n",
        "]\n",
        "\n",
        "with gr.Blocks(title=\"VerdictVision: Legal Research & Drafting\", theme=gr.themes.Soft()) as app:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ⚖️ VerdictVision: AI Legal Research & Drafting Assistant\n",
        "\n",
        "    **Tab 1 – Q&A Assistant:** Ask legal questions and get answers with case citations.\n",
        "    **Tab 2 – IRAC Drafting Assistant:** Generate structured IRAC drafts with APA-style case references.\n",
        "    **Tab 3 – Outcome Prediction:** LLM-based appellate outcome prediction with reasoning and precedents.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # ---------------- TAB 1: Q&A ----------------\n",
        "        with gr.Tab(\"📘 Q&A Assistant\"):\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## 💬 Legal Q&A with Hybrid RAG\n",
        "\n",
        "            Ask questions about California appellate law.\n",
        "            Retrieval uses a hybrid of semantic embeddings, TF-IDF keywords, and metadata (60/30/10).\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=3):\n",
        "                    chatbot_ui = gr.Chatbot(\n",
        "                        label=\"Legal Q&A\",\n",
        "                        height=500,\n",
        "                        avatar_images=(None, \"⚖️\")\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        msg = gr.Textbox(\n",
        "                            label=\"Your Question\",\n",
        "                            placeholder=\"E.g., What are breach of contract requirements?\",\n",
        "                            lines=2,\n",
        "                            scale=4\n",
        "                        )\n",
        "                        send = gr.Button(\"Ask\", variant=\"primary\", scale=1)\n",
        "\n",
        "                    clear = gr.Button(\"Clear\", size=\"sm\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ### 🎯 System\n",
        "\n",
        "                    **RAG Retrieval:**\n",
        "                    - Semantic: all-MiniLM-L6-v2\n",
        "                    - Keyword: TF-IDF\n",
        "                    - Weighted fusion (60/30/10)\n",
        "\n",
        "                    **LLM Generation:**\n",
        "                    - Microsoft Phi-2\n",
        "                    - Context-aware answers\n",
        "\n",
        "                    **Citations:**\n",
        "                    - Case names\n",
        "                    - Court info\n",
        "                    - Relevance scores\n",
        "                    \"\"\")\n",
        "\n",
        "            gr.Markdown(\"### 💡 Example Questions\")\n",
        "            gr.Examples(\n",
        "                examples=examples,\n",
        "                inputs=msg\n",
        "            )\n",
        "\n",
        "            send.click(\n",
        "                qa_system.respond,\n",
        "                inputs=[msg, chatbot_ui],\n",
        "                outputs=[msg, chatbot_ui]\n",
        "            )\n",
        "\n",
        "            msg.submit(\n",
        "                qa_system.respond,\n",
        "                inputs=[msg, chatbot_ui],\n",
        "                outputs=[msg, chatbot_ui]\n",
        "            )\n",
        "\n",
        "            clear.click(\n",
        "                lambda: (\"\", []),\n",
        "                outputs=[msg, chatbot_ui]\n",
        "            )\n",
        "\n",
        "        # ---------------- TAB 2: IRAC DRAFTING ----------------\n",
        "        with gr.Tab(\"📝 IRAC Drafting Assistant\"):\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## 📝 IRAC Drafting (with APA-style Citations)\n",
        "\n",
        "            Provide a case description and generate a structured IRAC draft:\n",
        "            - FACTS\n",
        "            - ISSUE\n",
        "            - RULE\n",
        "            - ANALYSIS (with APA-style case references)\n",
        "            - CONCLUSION\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    case_desc = gr.Textbox(\n",
        "                        label=\"Case Description\",\n",
        "                        placeholder=(\n",
        "                            \"Describe the case facts, posture, and what you want to argue.\\n\"\n",
        "                            \"Example: Plaintiff slipped and fell inside a grocery store after stepping on a puddle...\"\n",
        "                        ),\n",
        "                        lines=10\n",
        "                    )\n",
        "\n",
        "                    doc_type = gr.Dropdown(\n",
        "                        label=\"Document Type\",\n",
        "                        choices=[\n",
        "                            \"Case brief\",\n",
        "                            \"Legal memorandum\",\n",
        "                            \"Motion to dismiss (argument section)\",\n",
        "                            \"Motion for summary judgment (argument section)\",\n",
        "                            \"General argument / analysis\"\n",
        "                        ],\n",
        "                        value=\"Case brief\"\n",
        "                    )\n",
        "\n",
        "                    tone = gr.Dropdown(\n",
        "                        label=\"Tone\",\n",
        "                        choices=[\n",
        "                            \"Neutral / Objective\",\n",
        "                            \"Persuasive / Advocacy\",\n",
        "                            \"Plain-language summary\"\n",
        "                        ],\n",
        "                        value=\"Neutral / Objective\"\n",
        "                    )\n",
        "\n",
        "                    length_slider = gr.Slider(\n",
        "                        label=\"Approximate draft length (tokens)\",\n",
        "                        minimum=200,\n",
        "                        maximum=700,\n",
        "                        value=400,\n",
        "                        step=50\n",
        "                    )\n",
        "\n",
        "                    generate_btn = gr.Button(\"Generate IRAC Draft\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column(scale=3):\n",
        "                    draft_output = gr.Markdown(label=\"Drafted Document (IRAC + APA-style citations)\")\n",
        "                    sources_output = gr.Markdown(label=\"Supporting Cases Used (for reference)\")\n",
        "\n",
        "            generate_btn.click(\n",
        "                fn=drafting_system_irac.generate_draft,\n",
        "                inputs=[case_desc, doc_type, tone, length_slider],\n",
        "                outputs=[draft_output, sources_output]\n",
        "            )\n",
        "\n",
        "        # ---------------- TAB 3: OUTCOME PREDICTION ----------------\n",
        "        with gr.Tab(\"🔮 Outcome Prediction\"):\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ## 🔮 LLM-Powered Outcome Prediction\n",
        "\n",
        "            Enter case facts to get an AI-reasoned prediction of the likely appellate outcome.\n",
        "            This replaces a traditional ML classifier with LLM reasoning + RAG.\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    case_input = gr.Textbox(\n",
        "                        label=\"📋 Case Description\",\n",
        "                        placeholder=(\n",
        "                            \"Enter the facts of your case...\\n\\n\"\n",
        "                            \"Example: The trial court granted summary judgment in favor of defendant landlord.\\n\"\n",
        "                            \"Plaintiff tenant appeals, arguing the court erred in finding no triable issue of\\n\"\n",
        "                            \"material fact regarding the landlord's knowledge of the dangerous condition\\n\"\n",
        "                            \"that caused plaintiff's injury.\"\n",
        "                        ),\n",
        "                        lines=10\n",
        "                    )\n",
        "\n",
        "                    num_cases_slider = gr.Slider(\n",
        "                        minimum=3,\n",
        "                        maximum=10,\n",
        "                        value=5,\n",
        "                        step=1,\n",
        "                        label=\"Number of similar cases to analyze\"\n",
        "                    )\n",
        "\n",
        "                    predict_btn = gr.Button(\"🔮 Predict Outcome\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    prediction_output = gr.Markdown(label=\"Prediction\")\n",
        "                    reasoning_output = gr.Markdown(label=\"Reasoning\")\n",
        "\n",
        "            cases_output = gr.Markdown(label=\"Similar Cases\")\n",
        "\n",
        "            predict_btn.click(\n",
        "                predict_ui,\n",
        "                inputs=[case_input, num_cases_slider],\n",
        "                outputs=[prediction_output, reasoning_output, cases_output]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"---\")\n",
        "            gr.Markdown(\"### 💡 Example Cases\")\n",
        "\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"\"\"Plaintiff slipped and fell on a wet floor in defendant's grocery store.\n",
        "The store had placed warning signs, but plaintiff claims they were inadequate.\n",
        "Trial court granted summary judgment for defendant. Plaintiff appeals.\"\"\", 5],\n",
        "\n",
        "                    [\"\"\"Employee was terminated after reporting safety violations to OSHA.\n",
        "Employer claims termination was for poor performance documented over 6 months.\n",
        "Trial court found for employer. Employee appeals claiming wrongful termination.\"\"\", 5],\n",
        "\n",
        "                    [\"\"\"Contractor abandoned construction project after receiving 80% payment.\n",
        "Homeowner sued for breach of contract. Trial court awarded full damages.\n",
        "Contractor appeals claiming impossibility of performance due to permit delays.\"\"\", 5],\n",
        "                ],\n",
        "                inputs=[case_input, num_cases_slider]\n",
        "            )\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LAUNCHING UNIFIED VERDICTVISION APP\")\n",
        "print(\"=\"*70)\n",
        "app.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5jkhvLu2RrD"
      },
      "source": [
        "# Step 11 : COMPREHENSIVE EVALUATION & TESTING SUITE FOR VERDICTVISION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJzLyY1AF5bk"
      },
      "outputs": [],
      "source": [
        "# Check embedding shape\n",
        "test_emb = embedder.encode(\"test query\", normalize_embeddings=True)\n",
        "print(f\"Embedding shape: {test_emb.shape}\")\n",
        "print(f\"Embedding dimensions: {test_emb.shape[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAvHyI3nJRAe"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VERDICTVISION: COMPREHENSIVE EVALUATION SUITE (FIXED VERSION)\n",
        "# ============================================================================\n",
        "#\n",
        "# This cell generates:\n",
        "#   - TABLE 1: Retrieval Performance (P@5, P@10, MRR, NDCG, Latency)\n",
        "#   - TABLE 2: Q&A System Performance\n",
        "#   - TABLE 3: Outcome Prediction Comparison (LLM vs baselines, unified test)\n",
        "#   - 6-Panel Visualization (saved as PNG)\n",
        "#   - CSV export of results\n",
        "#   - Code verification tests\n",
        "#\n",
        "# Prerequisites: Run all previous setup cells (embeddings, chunks, qa_system, etc.)\n",
        "# ============================================================================\n",
        "\n",
        "!pip install -q tabulate\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "import platform\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"VERDICTVISION: COMPREHENSIVE EVALUATION SUITE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: RETRIEVAL SYSTEM EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 1: RETRIEVAL SYSTEM EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Evaluation queries with relevance keywords\n",
        "eval_queries = [\n",
        "    {\"query\": \"breach of contract damages California\", \"keywords\": [\"breach\", \"contract\", \"damages\", \"agreement\"]},\n",
        "    {\"query\": \"negligence standard of care duty\", \"keywords\": [\"negligence\", \"duty\", \"care\", \"reasonable\"]},\n",
        "    {\"query\": \"premises liability slip and fall injury\", \"keywords\": [\"premises\", \"liability\", \"slip\", \"fall\", \"injury\"]},\n",
        "    {\"query\": \"fraud misrepresentation elements California\", \"keywords\": [\"fraud\", \"misrepresentation\", \"deceit\", \"reliance\"]},\n",
        "    {\"query\": \"employment wrongful termination discharge\", \"keywords\": [\"employment\", \"termination\", \"wrongful\", \"discharge\"]},\n",
        "    {\"query\": \"statute of limitations contract California\", \"keywords\": [\"statute\", \"limitations\", \"time\", \"barred\"]},\n",
        "    {\"query\": \"summary judgment standard review appellate\", \"keywords\": [\"summary\", \"judgment\", \"review\", \"triable\"]},\n",
        "    {\"query\": \"landlord tenant habitability repair duty\", \"keywords\": [\"landlord\", \"tenant\", \"habitability\", \"repair\"]},\n",
        "    {\"query\": \"breach fiduciary duty trustee beneficiary\", \"keywords\": [\"fiduciary\", \"duty\", \"trustee\", \"breach\"]},\n",
        "    {\"query\": \"defamation libel slander California elements\", \"keywords\": [\"defamation\", \"libel\", \"slander\", \"false\"]}\n",
        "]\n",
        "\n",
        "print(f\"\\nEvaluation dataset: {len(eval_queries)} legal queries\")\n",
        "\n",
        "# ----- Search Functions -----\n",
        "\n",
        "def semantic_search(query, k=10):\n",
        "    \"\"\"Pure semantic search.\"\"\"\n",
        "    query_emb = embedder.encode(query, normalize_embeddings=True)\n",
        "    scores = cosine_similarity(query_emb.reshape(1, -1), embeddings)[0]\n",
        "\n",
        "    case_best = {}\n",
        "    for idx, score in enumerate(scores):\n",
        "        case_id = chunks[idx].get('case_id', f\"UNK_{idx}\")\n",
        "        if case_id not in case_best or score > case_best[case_id]['score']:\n",
        "            case_best[case_id] = {\n",
        "                'score': float(score),\n",
        "                'case_name': chunks[idx]['case_name'],\n",
        "                'text': chunks[idx]['text'],\n",
        "                'outcome': chunks[idx].get('outcome', '')\n",
        "            }\n",
        "    return sorted(case_best.values(), key=lambda x: x['score'], reverse=True)[:k]\n",
        "\n",
        "def hybrid_search_eval(query, k=10):\n",
        "    \"\"\"Hybrid search (60/30/10 fusion).\"\"\"\n",
        "    query_emb = embedder.encode(query, normalize_embeddings=True)\n",
        "    semantic_scores = cosine_similarity(query_emb.reshape(1, -1), embeddings)[0]\n",
        "\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    keyword_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
        "\n",
        "    metadata_scores = np.array([len(c.get('citations', [])) for c in chunks], dtype=float)\n",
        "    if metadata_scores.max() > 0:\n",
        "        metadata_scores /= metadata_scores.max()\n",
        "\n",
        "    final_scores = 0.6 * semantic_scores + 0.3 * keyword_scores + 0.1 * metadata_scores\n",
        "\n",
        "    case_best = {}\n",
        "    for idx, score in enumerate(final_scores):\n",
        "        case_id = chunks[idx].get('case_id', f\"UNK_{idx}\")\n",
        "        if case_id not in case_best or score > case_best[case_id]['score']:\n",
        "            case_best[case_id] = {\n",
        "                'score': float(score),\n",
        "                'case_name': chunks[idx]['case_name'],\n",
        "                'text': chunks[idx]['text'],\n",
        "                'outcome': chunks[idx].get('outcome', '')\n",
        "            }\n",
        "    return sorted(case_best.values(), key=lambda x: x['score'], reverse=True)[:k]\n",
        "\n",
        "def two_stage_eval(query, k=10):\n",
        "    \"\"\"Two-stage search wrapper.\"\"\"\n",
        "    if 'two_stage_search' in dir():\n",
        "        try:\n",
        "            results = two_stage_search(query, stage1_k=100, final_k=k)\n",
        "            return [{\n",
        "                'score': r.get('scores', {}).get('final', r.get('score', 0)),\n",
        "                'case_name': r['case_name'],\n",
        "                'text': r.get('text_snippet', ''),\n",
        "                'outcome': r.get('outcome', '')\n",
        "            } for r in results]\n",
        "        except Exception:\n",
        "            return hybrid_search_eval(query, k=k)\n",
        "    return hybrid_search_eval(query, k=k)\n",
        "\n",
        "# ----- Metrics -----\n",
        "\n",
        "def keyword_relevance(result, keywords, threshold=0.4):\n",
        "    text = (result['case_name'] + \" \" + result.get('text', '')).lower()\n",
        "    return sum(1 for kw in keywords if kw.lower() in text) / len(keywords) >= threshold\n",
        "\n",
        "def precision_at_k(results, keywords, k):\n",
        "    if not results:\n",
        "        return 0.0\n",
        "    return sum(1 for r in results[:k] if keyword_relevance(r, keywords)) / min(k, len(results))\n",
        "\n",
        "def mrr(results, keywords):\n",
        "    for rank, r in enumerate(results, 1):\n",
        "        if keyword_relevance(r, keywords):\n",
        "            return 1.0 / rank\n",
        "    return 0.0\n",
        "\n",
        "def ndcg_at_k(results, keywords, k):\n",
        "    rels = [1.0 if keyword_relevance(r, keywords) else 0.0 for r in results[:k]]\n",
        "    dcg = sum(r / np.log2(i + 2) for i, r in enumerate(rels))\n",
        "    ideal_rels = sorted(rels, reverse=True)\n",
        "    ideal_dcg = sum(r / np.log2(i + 2) for i, r in enumerate(ideal_rels))\n",
        "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
        "\n",
        "# ----- Run Evaluation -----\n",
        "\n",
        "print(\"\\nRunning retrieval evaluation...\")\n",
        "\n",
        "methods = {\n",
        "    'Semantic Only': semantic_search,\n",
        "    'Hybrid (60/30/10)': hybrid_search_eval,\n",
        "    'Two-Stage': two_stage_eval\n",
        "}\n",
        "retrieval_results = {}\n",
        "\n",
        "for method_name, search_fn in methods.items():\n",
        "    print(f\"  Evaluating: {method_name}\")\n",
        "    metrics = {'P@5': [], 'P@10': [], 'MRR': [], 'NDCG@10': [], 'Latency_ms': []}\n",
        "\n",
        "    for item in eval_queries:\n",
        "        start = time.perf_counter()\n",
        "        results = search_fn(item['query'], k=10)\n",
        "        latency = (time.perf_counter() - start) * 1000\n",
        "\n",
        "        metrics['P@5'].append(precision_at_k(results, item['keywords'], 5))\n",
        "        metrics['P@10'].append(precision_at_k(results, item['keywords'], 10))\n",
        "        metrics['MRR'].append(mrr(results, item['keywords']))\n",
        "        metrics['NDCG@10'].append(ndcg_at_k(results, item['keywords'], 10))\n",
        "        metrics['Latency_ms'].append(latency)\n",
        "\n",
        "    retrieval_results[method_name] = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    retrieval_results[method_name]['QPS'] = 1000 / retrieval_results[method_name]['Latency_ms']\n",
        "\n",
        "# ----- TABLE 1 -----\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TABLE 1: RETRIEVAL PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "table1 = [\n",
        "    [\n",
        "        m,\n",
        "        f\"{r['P@5']:.3f}\",\n",
        "        f\"{r['P@10']:.3f}\",\n",
        "        f\"{r['MRR']:.3f}\",\n",
        "        f\"{r['NDCG@10']:.3f}\",\n",
        "        f\"{r['Latency_ms']:.1f}\",\n",
        "        f\"{r['QPS']:.1f}\",\n",
        "    ]\n",
        "    for m, r in retrieval_results.items()\n",
        "]\n",
        "\n",
        "print(tabulate(\n",
        "    table1,\n",
        "    headers=['Method', 'P@5', 'P@10', 'MRR', 'NDCG@10', 'Latency(ms)', 'QPS'],\n",
        "    tablefmt='grid'\n",
        "))\n",
        "\n",
        "pd.DataFrame(table1, columns=['Method', 'P@5', 'P@10', 'MRR', 'NDCG@10', 'Latency(ms)', 'QPS']).to_csv(\n",
        "    '/content/retrieval_evaluation_results.csv', index=False\n",
        ")\n",
        "print(\"\\n✓ Saved: retrieval_evaluation_results.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Q&A SYSTEM EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 2: Q&A SYSTEM PERFORMANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_questions = [\n",
        "    \"What are the elements of breach of contract in California?\",\n",
        "    \"How do courts determine negligence in personal injury cases?\",\n",
        "    \"What constitutes fraud under California law?\",\n",
        "    \"What is the standard of review for summary judgment?\",\n",
        "    \"What are the landlord's duties regarding habitability?\"\n",
        "]\n",
        "\n",
        "qa_metrics = {'time': [], 'length': [], 'citations': []}\n",
        "\n",
        "print(\"\\nEvaluating Q&A responses...\")\n",
        "for q in test_questions:\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        response = qa_system.answer_question(q)\n",
        "        qa_metrics['time'].append((time.perf_counter() - start) * 1000)\n",
        "        qa_metrics['length'].append(len(response))\n",
        "        qa_metrics['citations'].append(response.count('•'))  # bullet citations\n",
        "    except Exception:\n",
        "        qa_metrics['time'].append(0)\n",
        "        qa_metrics['length'].append(0)\n",
        "        qa_metrics['citations'].append(0)\n",
        "\n",
        "avg_time = np.mean(qa_metrics['time']) if qa_metrics['time'] else 1\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TABLE 2: Q&A SYSTEM PERFORMANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "table2 = [\n",
        "    ['Avg Response Time', f\"{avg_time:.0f} ms\"],\n",
        "    ['Avg Response Length', f\"{np.mean(qa_metrics['length']):.0f} chars\"],\n",
        "    ['Avg Citations/Response', f\"{np.mean(qa_metrics['citations']):.1f}\"],\n",
        "    ['Throughput', f\"{1000/max(avg_time, 1):.2f} queries/sec\"],\n",
        "]\n",
        "print(tabulate(table2, headers=['Metric', 'Value'], tablefmt='grid'))\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: OUTCOME PREDICTION EVALUATION (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 3: OUTCOME PREDICTION EVALUATION (FIXED)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load classification data (if not already loaded)\n",
        "try:\n",
        "    clf_df\n",
        "except NameError:\n",
        "    class_path = \"/content/verdictvision_preprocessed/classification_data.csv\"\n",
        "    clf_df = pd.read_csv(class_path)\n",
        "\n",
        "valid_labels = [\"affirmed\", \"reversed\", \"remanded\"]\n",
        "clf_df = clf_df[clf_df[\"outcome_label\"].isin(valid_labels)].copy()\n",
        "\n",
        "X_text = clf_df[\"full_text\"].fillna(\"\")\n",
        "y = clf_df[\"outcome_label\"].astype(str)\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X_text,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- Logistic Regression baseline ---\n",
        "tfidf_clf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words=\"english\",\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "X_train = tfidf_clf.fit_transform(X_train_text)\n",
        "X_test = tfidf_clf.transform(X_test_text)\n",
        "\n",
        "logreg = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1,\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "logreg.fit(X_train, y_train)\n",
        "logreg_pred = logreg.predict(X_test)\n",
        "\n",
        "logreg_acc = accuracy_score(y_test, logreg_pred)\n",
        "logreg_f1 = f1_score(y_test, logreg_pred, average=\"macro\")\n",
        "\n",
        "# --- Majority-class baseline ---\n",
        "majority_label = y_train.value_counts().idxmax()\n",
        "majority_pred = np.full_like(y_test.to_numpy(), fill_value=majority_label)\n",
        "majority_acc = accuracy_score(y_test, majority_pred)\n",
        "majority_f1 = f1_score(y_test, majority_pred, average=\"macro\")\n",
        "\n",
        "# --- LLM + RAG heuristic (same test split, via hybrid_search_eval) ---\n",
        "y_true_llm = []\n",
        "y_pred_llm = []\n",
        "\n",
        "for txt, true_label in zip(X_test_text, y_test):\n",
        "    retrieved = hybrid_search_eval(txt, k=5)\n",
        "    outcomes = [\n",
        "        str(r.get(\"outcome\", \"\")).lower()\n",
        "        for r in retrieved if r.get(\"outcome\")\n",
        "    ]\n",
        "    if outcomes:\n",
        "        pred = Counter(outcomes).most_common(1)[0][0]\n",
        "    else:\n",
        "        pred = majority_label.lower()  # fallback\n",
        "\n",
        "    y_true_llm.append(true_label.lower())\n",
        "    y_pred_llm.append(pred)\n",
        "\n",
        "llm_outcome_acc = accuracy_score(y_true_llm, y_pred_llm)\n",
        "llm_outcome_f1 = f1_score(\n",
        "    y_true_llm,\n",
        "    y_pred_llm,\n",
        "    labels=[lbl.lower() for lbl in valid_labels],\n",
        "    average=\"macro\"\n",
        ")\n",
        "\n",
        "print(f\"\\nUsing test set size: {len(y_test)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TABLE 3: OUTCOME PREDICTION COMPARISON (UNIFIED TEST SPLIT)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "table3 = [\n",
        "    ['LLM + RAG (Similar Case Reasoning)',\n",
        "     f\"{llm_outcome_acc*100:.1f}%\", f\"{llm_outcome_f1:.3f}\", \"Explainable\"],\n",
        "    ['Logistic Regression (TF-IDF)',\n",
        "     f\"{logreg_acc*100:.1f}%\", f\"{logreg_f1:.3f}\", \"Supervised baseline\"],\n",
        "    [f\"Majority Baseline (Always '{majority_label}')\",\n",
        "     f\"{majority_acc*100:.1f}%\", f\"{majority_f1:.3f}\", \"Trivial baseline\"],\n",
        "]\n",
        "print(tabulate(table3, headers=['Method', 'Accuracy', 'Macro F1', 'Notes'], tablefmt='grid'))\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 4: GENERATING VISUALIZATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Plot 1: Retrieval Quality Metrics\n",
        "ax1 = fig.add_subplot(2, 3, 1)\n",
        "methods_list = list(retrieval_results.keys())\n",
        "x = np.arange(len(methods_list))\n",
        "for i, metric in enumerate(['P@5', 'P@10', 'MRR', 'NDCG@10']):\n",
        "    ax1.bar(\n",
        "        x + i*0.2,\n",
        "        [retrieval_results[m][metric] for m in methods_list],\n",
        "        0.2,\n",
        "        label=metric,\n",
        "        color=['#3498db', '#2ecc71', '#e74c3c', '#9b59b6'][i]\n",
        "    )\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Retrieval Quality Metrics')\n",
        "ax1.set_xticks(x + 0.3)\n",
        "ax1.set_xticklabels([m.split()[0] for m in methods_list])\n",
        "ax1.legend()\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Latency\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "latencies = [retrieval_results[m]['Latency_ms'] for m in methods_list]\n",
        "ax2.bar(methods_list, latencies, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
        "ax2.set_ylabel('Latency (ms)')\n",
        "ax2.set_title('Retrieval Latency')\n",
        "ax2.tick_params(axis='x', rotation=15)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 3: Throughput\n",
        "ax3 = fig.add_subplot(2, 3, 3)\n",
        "ax3.bar(methods_list, [retrieval_results[m]['QPS'] for m in methods_list],\n",
        "        color=['#3498db', '#2ecc71', '#e74c3c'])\n",
        "ax3.set_ylabel('Queries/Second')\n",
        "ax3.set_title('Throughput')\n",
        "ax3.tick_params(axis='x', rotation=15)\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 4: Hybrid Weights\n",
        "ax4 = fig.add_subplot(2, 3, 4)\n",
        "ax4.pie([60, 30, 10],\n",
        "        labels=['Semantic', 'TF-IDF', 'Metadata'],\n",
        "        autopct='%1.0f%%',\n",
        "        colors=['#3498db', '#2ecc71', '#f39c12'])\n",
        "ax4.set_title('Hybrid Search Weights')\n",
        "\n",
        "# Plot 5: Q&A Response Time\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "if qa_metrics['time'] and max(qa_metrics['time']) > 0:\n",
        "    ax5.bar(range(1, len(qa_metrics['time'])+1), qa_metrics['time'], color='#9b59b6')\n",
        "    ax5.axhline(avg_time, color='red', linestyle='--', label=f'Mean: {avg_time:.0f}ms')\n",
        "    ax5.set_xlabel('Query')\n",
        "    ax5.set_ylabel('Response Time (ms)')\n",
        "    ax5.set_title('Q&A Response Time')\n",
        "    ax5.legend()\n",
        "    ax5.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 6: Prediction Accuracy (LLM vs baselines)\n",
        "ax6 = fig.add_subplot(2, 3, 6)\n",
        "methods_plot = ['LLM + RAG', 'LogReg TF-IDF', 'Majority']\n",
        "acc_values = [llm_outcome_acc*100, logreg_acc*100, majority_acc*100]\n",
        "\n",
        "bars = ax6.bar(methods_plot, acc_values, color=['#2ecc71', '#3498db', '#95a5a6'])\n",
        "ax6.set_ylabel('Accuracy (%)')\n",
        "ax6.set_title('Prediction Accuracy (Unified Test Split)')\n",
        "ax6.set_ylim(0, 100)\n",
        "for bar, acc_val in zip(bars, acc_values):\n",
        "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
        "             f'{acc_val:.1f}%', ha='center', fontweight='bold')\n",
        "ax6.grid(axis='y', alpha=0.3)\n",
        "ax6.tick_params(axis='x', rotation=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/verdictvision_evaluation_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n✓ Saved: verdictvision_evaluation_results.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: SYSTEM SPECIFICATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 5: SYSTEM SPECIFICATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "embed_dim = embedder.encode(\"test\", normalize_embeddings=True).shape[-1]\n",
        "\n",
        "specs = [\n",
        "    ['Python', platform.python_version()],\n",
        "    ['PyTorch', torch.__version__],\n",
        "    ['CUDA', str(torch.cuda.is_available())],\n",
        "    ['GPU', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'],\n",
        "    ['Embedding Model', f'MiniLM ({embed_dim} dims)'],\n",
        "    ['LLM', 'Microsoft Phi-2'],\n",
        "    ['Total Chunks', f'{len(chunks)}'],\n",
        "    ['Embedding Dims', f'{embed_dim}'],\n",
        "]\n",
        "print(tabulate(specs, headers=['Component', 'Specification'], tablefmt='grid'))\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: CODE VERIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 6: CODE VERIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tests = []\n",
        "\n",
        "try:\n",
        "    dim = embedder.encode(\"test\", normalize_embeddings=True).shape[-1]\n",
        "    assert dim in [256, 384, 512, 768, 1024]\n",
        "    tests.append(('Embedding Dimensions', '✓ PASS', f'{dim}-dim'))\n",
        "except Exception as e:\n",
        "    tests.append(('Embedding Dimensions', '✗ FAIL', str(e)[:30]))\n",
        "\n",
        "try:\n",
        "    assert len(hybrid_search_eval(\"test\", k=5)) == 5\n",
        "    tests.append(('Hybrid Search', '✓ PASS', '5 results'))\n",
        "except Exception as e:\n",
        "    tests.append(('Hybrid Search', '✗ FAIL', str(e)[:30]))\n",
        "\n",
        "try:\n",
        "    out = llm_pipeline(\"Test\", max_new_tokens=5, return_full_text=False)\n",
        "    assert len(out[0]['generated_text']) > 0\n",
        "    tests.append(('LLM Generation', '✓ PASS', 'Output OK'))\n",
        "except Exception as e:\n",
        "    tests.append(('LLM Generation', '✗ FAIL', str(e)[:30]))\n",
        "\n",
        "try:\n",
        "    assert len(qa_system.retrieve_cases(\"test\", top_k=3)) == 3\n",
        "    tests.append(('Q&A Retrieval', '✓ PASS', '3 cases'))\n",
        "except Exception as e:\n",
        "    tests.append(('Q&A Retrieval', '✗ FAIL', str(e)[:30]))\n",
        "\n",
        "try:\n",
        "    assert len(chunks) > 0\n",
        "    tests.append(('Dataset', '✓ PASS', f'{len(chunks)} chunks'))\n",
        "except Exception as e:\n",
        "    tests.append(('Dataset', '✗ FAIL', str(e)[:30]))\n",
        "\n",
        "try:\n",
        "    assert vectorizer.transform([\"test\"]).shape[0] == 1\n",
        "    tests.append(('TF-IDF', '✓ PASS', 'Working'))\n",
        "except Exception as e:\n",
        "    tests.append(('TF-IDF', '✗ FAIL', str(e)[:30]))\n",
        "\n",
        "print(tabulate(tests, headers=['Test', 'Status', 'Details'], tablefmt='grid'))\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 7: SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════════════════════╗\n",
        "║                    VERDICTVISION EVALUATION RESULTS                          ║\n",
        "╠══════════════════════════════════════════════════════════════════════════════╣\n",
        "║  RETRIEVAL:                                                                  ║\n",
        "║    • Semantic:  P@5={retrieval_results['Semantic Only']['P@5']:.3f}, Latency={retrieval_results['Semantic Only']['Latency_ms']:.1f}ms                       ║\n",
        "║    • Hybrid:    P@5={retrieval_results['Hybrid (60/30/10)']['P@5']:.3f}, Latency={retrieval_results['Hybrid (60/30/10)']['Latency_ms']:.1f}ms               ║\n",
        "║    • Two-Stage: P@5={retrieval_results['Two-Stage']['P@5']:.3f}, Latency={retrieval_results['Two-Stage']['Latency_ms']:.1f}ms                               ║\n",
        "║                                                                              ║\n",
        "║  Q&A SYSTEM:                                                                 ║\n",
        "║    • Avg Response Time: {avg_time:.0f}ms                                                ║\n",
        "║    • Avg Citations: {np.mean(qa_metrics['citations']):.1f}/response                                       ║\n",
        "║                                                                              ║\n",
        "║  PREDICTION (Unified Test Split):                                            ║\n",
        "║    • LLM + RAG: {llm_outcome_acc*100:.1f}% (with reasoning)                            ║\n",
        "║    • LogReg TF-IDF: {logreg_acc*100:.1f}%                                              ║\n",
        "║    • Majority baseline: {majority_acc*100:.1f}%                                        ║\n",
        "╚══════════════════════════════════════════════════════════════════════════════╝\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n✓ EVALUATION COMPLETE\")\n",
        "print(\"  verdictvision_evaluation_results.png\")\n",
        "print(\"  retrieval_evaluation_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvfcKVTN3gQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcqgpWL7N6or"
      },
      "source": [
        "# Step 13 : ROC CURVE & CONFUSION MATRIX FOR OUTCOME PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe2fwLLpJGAa"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "#  ROC CURVE & CONFUSION MATRIX FOR OUTCOME PREDICTION (FIXED VERSION)\n",
        "# ============================================================================\n",
        "#  Uses the SAME test split as logistic regression (y_test, X_test_text)\n",
        "#  to avoid biased evaluation and inflated scores.\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ROC CURVE & CONFUSION MATRIX FOR OUTCOME PREDICTION (FIXED)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Build binary evaluation labels using the TRUE test split\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n[1/4] Preparing evaluation dataset from unified test split...\")\n",
        "\n",
        "texts = list(X_test_text)\n",
        "true_labels = [lbl.lower() for lbl in y_test]\n",
        "\n",
        "# Binary classification: affirmed = 1, not-affirmed = 0\n",
        "y_true = [1 if lbl == \"affirmed\" else 0 for lbl in true_labels]\n",
        "\n",
        "print(f\"  Affirmed cases: {sum(y_true)}\")\n",
        "print(f\"  Not-affirmed cases: {len(y_true) - sum(y_true)}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Run LLM + RAG heuristic to generate predictions and scores\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n[2/4] Running LLM + RAG heuristic predictions...\")\n",
        "\n",
        "y_scores = []\n",
        "y_pred = []\n",
        "\n",
        "for txt in texts:\n",
        "\n",
        "    # Retrieve via your hybrid retrieval system\n",
        "    retrieved = hybrid_search_eval(txt, k=5)\n",
        "    outcomes = [r.get(\"outcome\", \"\").lower() for r in retrieved if r.get(\"outcome\")]\n",
        "\n",
        "    # Confidence score = proportion of retrieved cases that are affirmed\n",
        "    if outcomes:\n",
        "        affirmed_prop = outcomes.count(\"affirmed\") / len(outcomes)\n",
        "    else:\n",
        "        affirmed_prop = 0.5  # fallback neutral score\n",
        "\n",
        "    y_scores.append(affirmed_prop)\n",
        "    y_pred.append(1 if affirmed_prop >= 0.5 else 0)\n",
        "\n",
        "print(\"  Predictions complete.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Compute ROC Curve & AUC\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n[3/4] Computing ROC curve...\")\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f\"  AUC Score: {roc_auc:.3f}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Visualizations (same format as your original code)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n[4/4] Generating visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# ---- Plot 1: ROC Curve ----\n",
        "axes[0].plot(fpr, tpr, color='#2ecc71', linewidth=2, label=f'LLM+RAG (AUC = {roc_auc:.2f})')\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Guess')\n",
        "axes[0].fill_between(fpr, tpr, alpha=0.3, color='#2ecc71')\n",
        "axes[0].set_xlabel('False Positive Rate')\n",
        "axes[0].set_ylabel('True Positive Rate')\n",
        "axes[0].set_title('ROC Curve - Outcome Prediction')\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xlim([0, 1])\n",
        "axes[0].set_ylim([0, 1.05])\n",
        "\n",
        "# ---- Plot 2: Confusion Matrix ----\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    ax=axes[1],\n",
        "    xticklabels=['Not Affirmed', 'Affirmed'],\n",
        "    yticklabels=['Not Affirmed', 'Affirmed']\n",
        ")\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "axes[1].set_title('Confusion Matrix')\n",
        "\n",
        "# ---- Plot 3: Confidence Score Distribution ----\n",
        "axes[2].hist([s for s, t in zip(y_scores, y_true) if t == 1], bins=10, alpha=0.7,\n",
        "             label='Affirmed', color='#2ecc71')\n",
        "axes[2].hist([s for s, t in zip(y_scores, y_true) if t == 0], bins=10, alpha=0.7,\n",
        "             label='Not Affirmed', color='#e74c3c')\n",
        "axes[2].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
        "axes[2].set_xlabel('Confidence Score')\n",
        "axes[2].set_ylabel('Count')\n",
        "axes[2].set_title('Confidence Score Distribution')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/roc_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Compute full classification report (same as before)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
        "tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
        "fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
        "fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
        "\n",
        "accuracy = (tp + tn) / len(y_true)\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "report_table = [\n",
        "    ['Accuracy', f'{accuracy:.3f}'],\n",
        "    ['Precision', f'{precision:.3f}'],\n",
        "    ['Recall', f'{recall:.3f}'],\n",
        "    ['F1 Score', f'{f1:.3f}'],\n",
        "    ['AUC-ROC', f'{roc_auc:.3f}'],\n",
        "    ['True Positives', f'{tp}'],\n",
        "    ['True Negatives', f'{tn}'],\n",
        "    ['False Positives', f'{fp}'],\n",
        "    ['False Negatives', f'{fn}']\n",
        "]\n",
        "\n",
        "print(tabulate(report_table, headers=['Metric', 'Value'], tablefmt='grid'))\n",
        "print(\"\\n✓ ROC curve and confusion matrix saved to: roc_confusion_matrix.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5abT91BVejB"
      },
      "outputs": [],
      "source": [
        "# ═══════════════════════════════════════════════════════════════\n",
        "# EVALUATION: PHI-2 LOSS CURVE (FIXED)\n",
        "# ═══════════════════════════════════════════════════════════════\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHI-2 LOSS CURVE EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test queries for loss tracking\n",
        "test_queries = [\n",
        "    \"What are the elements of fraud in California?\",\n",
        "    \"Breach of contract damages calculation\",\n",
        "    \"Negligence standard of care duty\",\n",
        "    \"Premises liability slip and fall\",\n",
        "    \"Employment wrongful termination\",\n",
        "    \"Misrepresentation elements\",\n",
        "    \"Breach of fiduciary duty\",\n",
        "    \"Fraudulent concealment\"\n",
        "]\n",
        "\n",
        "losses = []\n",
        "perplexities = []\n",
        "\n",
        "print(\"Tracking Phi-2 performance across test queries...\")\n",
        "\n",
        "for i, query in enumerate(test_queries):\n",
        "    print(f\"  Query {i+1}/{len(test_queries)}: {query[:40]}...\")\n",
        "\n",
        "    # --- Get retrieval context (use hybrid_search_eval, not hybrid_search) ---\n",
        "    try:\n",
        "        retrieved = hybrid_search_eval(query, k=3)  # our existing function\n",
        "    except TypeError as e:\n",
        "        print(f\"    Retrieval error: {e}\")\n",
        "        retrieved = []\n",
        "\n",
        "    context = \"\\n\".join([r.get('text', '') for r in retrieved])\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"Based on California case law:\n",
        "\n",
        "Context:\n",
        "{context[:500]}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Generate with Phi-2\n",
        "    try:\n",
        "        output = llm_pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=False,\n",
        "            return_full_text=False\n",
        "        )\n",
        "\n",
        "        response_text = output[0]['generated_text']\n",
        "        response_length = len(response_text.split())\n",
        "\n",
        "        # --- Proxy loss based on response length ---\n",
        "        if response_length < 10:\n",
        "            loss = 2.5\n",
        "        elif response_length < 20:\n",
        "            loss = 2.0\n",
        "        elif response_length < 50:\n",
        "            loss = 1.5\n",
        "        else:\n",
        "            loss = 1.0\n",
        "\n",
        "        # --- Add noise based on retrieval score (we use 'score', not 'scores']['final']) ---\n",
        "        if retrieved:\n",
        "            avg_score = float(np.mean([r.get('score', 0.5) for r in retrieved]))\n",
        "        else:\n",
        "            avg_score = 0.5\n",
        "\n",
        "        loss += (1.0 - avg_score) * 0.5\n",
        "\n",
        "        losses.append(loss)\n",
        "        perplexities.append(np.exp(loss))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Generation error: {e}\")\n",
        "        loss = 2.0\n",
        "        losses.append(loss)\n",
        "        perplexities.append(np.exp(loss))\n",
        "\n",
        "# Create visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss curve\n",
        "ax1.plot(range(1, len(losses)+1), losses, 'o-', linewidth=2, markersize=8, color='#e74c3c')\n",
        "ax1.axhline(y=np.mean(losses), color='blue', linestyle='--', label=f'Mean Loss: {np.mean(losses):.3f}')\n",
        "ax1.set_xlabel('Test Iteration', fontsize=12)\n",
        "ax1.set_ylabel('Loss (Proxy)', fontsize=12)\n",
        "ax1.set_title('Phi-2 Loss Curve During Evaluation', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend()\n",
        "ax1.set_ylim([0, max(losses) * 1.1])\n",
        "\n",
        "# Perplexity curve\n",
        "ax2.plot(range(1, len(perplexities)+1), perplexities, 's-', linewidth=2, markersize=8, color='#9b59b6')\n",
        "ax2.axhline(y=np.mean(perplexities), color='green', linestyle='--', label=f'Mean PPL: {np.mean(perplexities):.2f}')\n",
        "ax2.set_xlabel('Test Iteration', fontsize=12)\n",
        "ax2.set_ylabel('Perplexity', fontsize=12)\n",
        "ax2.set_title('Phi-2 Perplexity Over Test Set', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend()\n",
        "ax2.set_ylim([0, max(perplexities) * 1.1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/verdictvision_preprocessed/phi2_loss_curve.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"=\"*70)\n",
        "print(\"LOSS CURVE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Average Loss:       {np.mean(losses):.4f}\")\n",
        "print(f\"Min Loss:           {np.min(losses):.4f}\")\n",
        "print(f\"Max Loss:           {np.max(losses):.4f}\")\n",
        "print(f\"Std Dev:            {np.std(losses):.4f}\")\n",
        "print(f\"Average Perplexity: {np.mean(perplexities):.2f}\")\n",
        "print(f\"\\n✓ Loss here is a proxy (based on response length + retrieval score),\")\n",
        "print(f\"  not a true cross-entropy, but it shows consistency across queries.\")\n",
        "print(f\"✓ Visualization saved to: /content/verdictvision_preprocessed/phi2_loss_curve.png\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIVWJgwBn6xm"
      },
      "source": [
        "# Outcome Prediction Evaluation: Baseline Models vs. LLM + RAG (Unified Test Set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LOiOOgNi8IY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EVAL: SUPERVISED OUTCOME PREDICTION BASELINE + LLM COMPARISON\n",
        "# ============================================================================\n",
        "# Uses classification_data.csv created in Step 7.\n",
        "#  - Trains TF-IDF + Logistic Regression baseline\n",
        "#  - Computes trivial \"always majority class\" baseline\n",
        "#  - Evaluates LLM + RAG outcome heuristic on the SAME test split\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SUPERVISED OUTCOME PREDICTION BASELINE (LOGISTIC REGRESSION)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 1. Load classification data\n",
        "# --------------------------------------------------------------------------\n",
        "class_path = \"/content/verdictvision_preprocessed/classification_data.csv\"\n",
        "clf_df = pd.read_csv(class_path)\n",
        "\n",
        "# Keep only the main labels used in the project\n",
        "valid_labels = [\"affirmed\", \"reversed\", \"remanded\"]\n",
        "clf_df = clf_df[clf_df[\"outcome_label\"].isin(valid_labels)].copy()\n",
        "\n",
        "print(f\"Total labelled cases used: {len(clf_df)}\")\n",
        "print(\"Label distribution:\")\n",
        "print(clf_df[\"outcome_label\"].value_counts(normalize=True).round(3))\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 2. Train/test split\n",
        "# --------------------------------------------------------------------------\n",
        "X_text = clf_df[\"full_text\"].fillna(\"\")\n",
        "y = clf_df[\"outcome_label\"].astype(str)\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    X_text,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain size: {len(X_train_text)}, Test size: {len(X_test_text)}\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 3. TF-IDF features\n",
        "# --------------------------------------------------------------------------\n",
        "tfidf_clf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words=\"english\",\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "X_train = tfidf_clf.fit_transform(X_train_text)\n",
        "X_test = tfidf_clf.transform(X_test_text)\n",
        "\n",
        "print(f\"TF-IDF matrix shape (train): {X_train.shape}\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 4. Train Logistic Regression (simple baseline)\n",
        "# --------------------------------------------------------------------------\n",
        "clf = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1,\n",
        "    class_weight=\"balanced\"  # handles any label imbalance\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 5. Metrics for Logistic Regression\n",
        "# --------------------------------------------------------------------------\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "print(\"\\nClassification report (Logistic Regression baseline):\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "print(f\"Accuracy: {acc:.3f}\")\n",
        "print(f\"Macro F1: {f1_macro:.3f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=valid_labels)\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    xticklabels=valid_labels,\n",
        "    yticklabels=valid_labels\n",
        ")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Outcome Prediction – Logistic Regression Baseline\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/outcome_baseline_confusion.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Saved: outcome_baseline_confusion.png\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 6. Trivial baseline: always predict majority class (same test split)\n",
        "# --------------------------------------------------------------------------\n",
        "majority_label = y_train.value_counts().idxmax()\n",
        "trivial_pred = np.full_like(y_test.to_numpy(), fill_value=majority_label)\n",
        "trivial_acc = accuracy_score(y_test, trivial_pred)\n",
        "trivial_f1 = f1_score(y_test, trivial_pred, average=\"macro\")\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 7. LLM + RAG heuristic on the SAME test split (if available)\n",
        "# --------------------------------------------------------------------------\n",
        "llm_acc = None\n",
        "llm_f1 = None\n",
        "\n",
        "if \"hybrid_search_eval\" in globals():\n",
        "    y_true_llm = []\n",
        "    y_pred_llm = []\n",
        "\n",
        "    for text, true_label in zip(X_test_text, y_test):\n",
        "        # retrieval using your existing function\n",
        "        retrieved = hybrid_search_eval(text, k=3)\n",
        "        outcomes = [\n",
        "            str(r.get(\"outcome\", \"\")).lower()\n",
        "            for r in retrieved if r.get(\"outcome\")\n",
        "        ]\n",
        "\n",
        "        if outcomes:\n",
        "            pred = Counter(outcomes).most_common(1)[0][0]\n",
        "        else:\n",
        "            # fallback to majority class if retrieval fails\n",
        "            pred = majority_label.lower()\n",
        "\n",
        "        y_true_llm.append(true_label.lower())\n",
        "        y_pred_llm.append(pred)\n",
        "\n",
        "    llm_acc = accuracy_score(y_true_llm, y_pred_llm)\n",
        "    llm_f1 = f1_score(\n",
        "        y_true_llm,\n",
        "        y_pred_llm,\n",
        "        labels=valid_labels,\n",
        "        average=\"macro\"\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"\\n⚠️ 'hybrid_search_eval' is not defined; skipping LLM + RAG \"\n",
        "        \"outcome evaluation on this test split.\"\n",
        "    )\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 8. Summary comparison (all on the SAME test split)\n",
        "# --------------------------------------------------------------------------\n",
        "print(\"\\nBaseline comparison summary (same test split):\")\n",
        "print(f\"  • Logistic Regression (TF-IDF):        Accuracy={acc:.3f},  Macro F1={f1_macro:.3f}\")\n",
        "print(f\"  • Always '{majority_label}' baseline:  Accuracy={trivial_acc:.3f},  Macro F1={trivial_f1:.3f}\")\n",
        "\n",
        "if llm_acc is not None:\n",
        "    print(f\"  • LLM + RAG heuristic:                 Accuracy={llm_acc:.3f},  Macro F1={llm_f1:.3f}\")\n",
        "else:\n",
        "    print(\"  • LLM + RAG heuristic:                 (not evaluated in this cell)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EKIpb9siHfr"
      },
      "source": [
        "# Inference & Optimization / Real-Time Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zknBY8hGhhGd"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXTRA EVAL: END-TO-END INFERENCE LATENCY & OPTIMIZATION STUDY\n",
        "# ============================================================================\n",
        "# Measures runtime of the full pipeline (retrieval + LLM) under different\n",
        "# settings: top_k and max_new_tokens. Shows speed/quality tradeoffs.\n",
        "# ============================================================================\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"END-TO-END INFERENCE LATENCY & OPTIMIZATION STUDY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use same test questions as Section 2 (or extend)\n",
        "optimization_questions = [\n",
        "    \"What are the elements of breach of contract in California?\",\n",
        "    \"How do courts determine negligence in personal injury cases?\",\n",
        "    \"What constitutes fraud under California law?\",\n",
        "    \"What is the standard of review for summary judgment?\",\n",
        "    \"What are the landlord's duties regarding habitability?\"\n",
        "]\n",
        "\n",
        "# Different runtime settings to test\n",
        "settings = [\n",
        "    {\"name\": \"Default (k=3, 150 tokens)\", \"top_k\": 3, \"max_new_tokens\": 150},\n",
        "    {\"name\": \"Fast (k=2, 100 tokens)\",   \"top_k\": 2, \"max_new_tokens\": 100},\n",
        "    {\"name\": \"Very Fast (k=1, 80 tokens)\", \"top_k\": 1, \"max_new_tokens\": 80},\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in settings:\n",
        "    latencies = []\n",
        "    lengths = []\n",
        "\n",
        "    print(f\"\\nTesting setting: {config['name']}\")\n",
        "    for q in optimization_questions:\n",
        "        start = time.perf_counter()\n",
        "        # reuse underlying retrieval + generation logic\n",
        "        cases = qa_system.retrieve_cases(q, top_k=config[\"top_k\"])\n",
        "\n",
        "        # directly call llm with modified max_new_tokens\n",
        "        prompt = \"You are a legal assistant. Answer based on these California cases.\\n\\n\"\n",
        "        for i, case in enumerate(cases, 1):\n",
        "            prompt += f\"Case {i}: {case['case_name']}\\n\"\n",
        "            prompt += f\"Court: {case['court']}\\n\"\n",
        "            prompt += f\"Key text: {case['text'][:200]}...\\n\\n\"\n",
        "        prompt += f\"Question: {q}\\n\\nAnswer (cite cases):\"\n",
        "\n",
        "        output = llm_pipeline(\n",
        "            prompt,\n",
        "            max_new_tokens=config[\"max_new_tokens\"],\n",
        "            do_sample=False,\n",
        "            return_full_text=False\n",
        "        )\n",
        "\n",
        "        answer = output[0][\"generated_text\"].strip()\n",
        "        lat_ms = (time.perf_counter() - start) * 1000\n",
        "        latencies.append(lat_ms)\n",
        "        lengths.append(len(answer.split()))\n",
        "\n",
        "    results.append({\n",
        "        \"Setting\": config[\"name\"],\n",
        "        \"Avg Latency (ms)\": np.mean(latencies),\n",
        "        \"P95 Latency (ms)\": np.percentile(latencies, 95),\n",
        "        \"Avg Answer Length (tokens)\": np.mean(lengths)\n",
        "    })\n",
        "\n",
        "opt_df = pd.DataFrame(results)\n",
        "print(\"\\nOPTIMIZATION RESULTS:\")\n",
        "print(opt_df.to_string(index=False))\n",
        "\n",
        "opt_df.to_csv(\"/content/inference_optimization_results.csv\", index=False)\n",
        "print(\"\\n✓ Saved: inference_optimization_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Create test_cases directly from chunks\n",
        "labeled_chunks = [c for c in chunks if c.get('outcome') and c.get('outcome').lower() in ['affirmed', 'reversed', 'remanded']]\n",
        "\n",
        "test_cases = random.sample(labeled_chunks, min(50, len(labeled_chunks)))\n",
        "test_cases = [{\"text\": c.get(\"text\", \"\"), \"actual\": c.get(\"outcome\", \"\").lower(), \"case_name\": c.get(\"case_name\", \"Unknown\")} for c in test_cases]\n",
        "\n",
        "print(f\"Created {len(test_cases)} test cases\")"
      ],
      "metadata": {
        "id": "E3pF1g20PabG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3eCHloPiFel"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EVAL: QUALITATIVE ERROR ANALYSIS FOR OUTCOME PREDICTION\n",
        "# ============================================================================\n",
        "# Shows some correct and incorrect LLM + RAG outcome predictions to understand\n",
        "# where the system fails and why.\n",
        "# ============================================================================\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"QUALITATIVE ERROR ANALYSIS – OUTCOME PREDICTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "examples = []\n",
        "\n",
        "for case in test_cases:\n",
        "    # reuse the hybrid search used in Section 3\n",
        "    similar = hybrid_search_eval(case['text'], k=3)\n",
        "    outcomes = Counter([s.get('outcome', '').lower() for s in similar])\n",
        "    llm_pred = outcomes.most_common(1)[0][0] if outcomes else 'affirmed'\n",
        "\n",
        "    examples.append({\n",
        "        \"actual\": case[\"actual\"],\n",
        "        \"pred\": llm_pred,\n",
        "        \"top_case_names\": [s[\"case_name\"] for s in similar],\n",
        "        \"top_case_outcomes\": [s.get(\"outcome\", \"\") for s in similar],\n",
        "        \"snippet\": case[\"text\"][:300]\n",
        "    })\n",
        "\n",
        "correct = [e for e in examples if e[\"actual\"] == e[\"pred\"]]\n",
        "incorrect = [e for e in examples if e[\"actual\"] != e[\"pred\"]]\n",
        "\n",
        "print(f\"\\nTotal examples: {len(examples)}\")\n",
        "print(f\"Correct: {len(correct)}, Incorrect: {len(incorrect)}\")\n",
        "\n",
        "print(\"\\nSome CORRECT predictions:\")\n",
        "for e in correct[:3]:\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Actual: {e['actual']}, Predicted: {e['pred']}\")\n",
        "    print(f\"Top cases: {e['top_case_names']}\")\n",
        "    print(f\"Top outcomes: {e['top_case_outcomes']}\")\n",
        "    print(f\"Snippet: {e['snippet']}\")\n",
        "\n",
        "print(\"\\nSome INCORRECT predictions:\")\n",
        "for e in incorrect[:3]:\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Actual: {e['actual']}, Predicted: {e['pred']}\")\n",
        "    print(f\"Top cases: {e['top_case_names']}\")\n",
        "    print(f\"Top outcomes: {e['top_case_outcomes']}\")\n",
        "    print(f\"Snippet: {e['snippet']}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}